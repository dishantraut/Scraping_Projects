{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Collecting Data from all 9 Categories , Cleanning it and storing it in the form of csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TO GET CURRENT DATE \"\"\"\n",
    "def to_date():\n",
    "    try:\n",
    "            today = date.today()\n",
    "            demo = today.strftime(\"%d/%m/%Y\")\n",
    "            day = int(demo.split(\"/\")[0])\n",
    "            day = str(day-1)\n",
    "            month = demo.split(\"/\")[1]\n",
    "            year = demo.split(\"/\")[2]\n",
    "            date_str = month+\"/\"+day+\"/\"+year\n",
    "            return date_str\n",
    "    except Exception as e:\n",
    "        print(\"Error at to_date() = \",e)\n",
    "\n",
    "\"\"\" TO LOGIN \"\"\"\n",
    "def login(F_Link):\n",
    "    try:\n",
    "        global no_cases\n",
    "        driver.get(\"https://app02.clerk.org/ccms\")\n",
    "        time.sleep(10)\n",
    "        \"\"\"####\"\"\"\n",
    "        driver.find_element_by_xpath(\"//*[@id='ctl00_Content1_button_accept']\").click()\n",
    "        time.sleep(4)\n",
    "        driver.find_element_by_xpath(\"//*[@id='ctl00_Content1_caseType']/option[3]\").click()\n",
    "        driver.find_element_by_id('ctl00_Content1_fileStartDate').send_keys(\"03/16/2020\")\n",
    "        driver.find_element_by_id('ctl00_Content1_fileEndDate').send_keys(to_date_str)\n",
    "        time.sleep(4)\n",
    "        driver.find_element_by_xpath(F_Link).click()\n",
    "        driver.find_element_by_id(\"ctl00_Content1_lb_submit\").click()\n",
    "        time.sleep(3)\n",
    "        no_cases_str = driver.find_element_by_xpath('//*[@id=\"ctl00_Content1_lblResults\"]').text\n",
    "        \n",
    "        #For geting number of cases\n",
    "        try:\n",
    "            no_cases = int(no_cases_str.split(\" \")[2])\n",
    "            time.sleep(1)\n",
    "        except IndexError:\n",
    "            print(\"Error at login(F_Link) = *no_cases* point\")\n",
    "            print(\"*********BURNNING THE CODE AGAIN*********\")\n",
    "        except Exception as e:\n",
    "            print(\"N-H  Exception\\tError at login(F_Link) = *no_cases* point\")\n",
    "            print(\"N-H  Exception = \",e)\n",
    "            print(\"*********BURN THE CODE AGAIN*********\")\n",
    "        \n",
    "    except NoSuchElementException :\n",
    "        print(\"\\nError at login() = NoSuchElementException\")\n",
    "        print(\"*********BURNNING THE CODE AGAIN*********\")\n",
    "    except WebDriverException :\n",
    "        print(\"\\nError at login() = WebDriverException\")\n",
    "        print(\"*********BURN THE CODE AGAIN*********\")\n",
    "    except ElementClickInterceptedException :\n",
    "        print(\"\\nChange the Proxy IP ... Crawler Got Detected ...\")\n",
    "        print(\"Run the code Again...\")\n",
    "    except Exception as e:\n",
    "        print(\"\\nN-H  Exception = \",e)\n",
    "        print(\"*********BURN THE CODE AGAIN*********\")\n",
    "\n",
    "\n",
    "\"\"\" COLLECT DATA FROM SOUP \"\"\"\n",
    "def data_collection_from_soup():\n",
    "    try:\n",
    "\n",
    "        #get TEXT out of SOUP\n",
    "        #global txt\n",
    "        txt=[]\n",
    "        for row in soup.findAll('tr',attrs={'class':'altrowstyle'}):\n",
    "            data = row.text\n",
    "            txt.append(data)\n",
    "        for row in soup.findAll('tr',attrs={'class':'rowstyle'}):\n",
    "            data = row.text\n",
    "            txt.append(data)\n",
    "\n",
    "\n",
    "        # DEFENDENT_NAME_LIST and CASE_DATE_LIST  dd/mm//yyy format\n",
    "        #n_case_date=[]\n",
    "        #defendent_name_l=[]\n",
    "        for i in txt:\n",
    "            new_str=re.search(\"v.+?ET AL\",i).group()\n",
    "            defendent_name = new_str.split(\".\")[1].split(\",\")[0]\n",
    "            defendent_name_l.append(defendent_name)\n",
    "            #print(defendent_name)\n",
    "            ###################\n",
    "            case_date=re.search(\"\\d{2}\\/\\d{2}\\/\\d{4}\",i).group()\n",
    "            demo=case_date.split(\"/\")\n",
    "            new_case_date = demo[1]+\"/\"+demo[0]+\"/\"+demo[2]\n",
    "            n_case_date.append(new_case_date)\n",
    "\n",
    "\n",
    "        #BANK_NAME_LIST\n",
    "        #bank_name = []\n",
    "        for i in txt:\n",
    "            try:\n",
    "                demo=i.split(\"\\n\")[7].split(\"v.\")[0]\n",
    "                #bank_name.append(b_n)\n",
    "                if(\",\" in demo):\n",
    "                    f_bank_name.append(demo.split(\",\")[0])\n",
    "                else:\n",
    "                    f_bank_name.append(demo)            \n",
    "            except Exception as e:\n",
    "                demo=i.split(\"\\n\")[7].split(\"v.\")[0]\n",
    "                f_bank_name.append(demo)\n",
    "\n",
    "\n",
    "        #FINAL_CASE_ID_LIST \n",
    "        #f_case_id=[]\n",
    "        try:\n",
    "            for i in range(len(txt)):\n",
    "                x=re.search(\"\\d{4}\",txt[i]).group()\n",
    "                demo=re.search(\"\\\\xa0\\d{5}\",txt[i]).group()\n",
    "                y=demo.replace(\"\\xa0\",\"\")\n",
    "                demo=re.search(\"(\\\\xa0CICI)|(\\\\xa0CIDL)\",txt[i]).group()\n",
    "                z=demo.replace(\"\\xa0\",\"\")\n",
    "                f_case_id.append(x+\" \"+y+\" \"+z)\n",
    "        except Exception as e:\n",
    "            print(\"Error at FINAL_CASE_ID_LIST = \",e)\n",
    "\n",
    "        \"\"\"check for list equality in all list \"\"\"\n",
    "        if ( len(defendent_name_l)==len(f_case_id)==len(n_case_date)==len(f_bank_name) ):\n",
    "            print(len(defendent_name_l),len(f_case_id),len(n_case_date),len(f_bank_name))\n",
    "            #print(defendent_name_l,f_case_id,txt,n_case_date,bank_name,href_link)\n",
    "            print(\"YOO MAN ... !! all data of equal lenght\")\n",
    "        else:\n",
    "            print(\"NOT equal lenght\")\n",
    "            print(len(defendent_name_l),len(f_case_id),len(n_case_date),len(f_bank_name))\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error at data_collection_from_soup() = \",e)\n",
    "        print(\"Check the Function....  -  data_collection_from_soup()  \")\n",
    "        \n",
    "        \n",
    "####################################################################################\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from pdf2image import convert_from_path\n",
    "# import win32com.client as wincl       # Speech Module\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from PIL import Image \n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "import pyautogui\n",
    "import requests\n",
    "import random\n",
    "import PyPDF2 \n",
    "import json\n",
    "import mouse\n",
    "import time \n",
    "import os\n",
    "import re\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\\\Program Files (x86)\\\\Tesseract-OCR\\\\tesseract.exe\"\n",
    "base_dir = os.path.abspath(os.path.dirname(\"__file__\"))\n",
    "\n",
    "# Different Proxys : =    192.41.71.199:3128   50.246.120.125:8080     \n",
    "PROXY =     \"192.41.71.199:3128\" #\"74.121.98.90:8080\"      #\"192.144.215.244:3128\"  \n",
    "            #\"157.230.44.213:8080\"   #\"74.121.98.90:8080\"    #\"192.41.71.221:3128\"\n",
    "options = webdriver.ChromeOptions()\n",
    "appState = {\n",
    "    \"recentDestinations\": [\n",
    "        {\n",
    "            \"id\": \"Save as PDF\",\n",
    "            \"origin\": \"local\",\n",
    "            \"account\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    \"selectedDestinationId\": \"Save as PDF\",\n",
    "    \"version\": 2\n",
    "}\n",
    "profile = {'printing.print_preview_sticky_settings.appState': json.dumps(appState)}\n",
    "options.add_experimental_option('prefs',  {\n",
    "    \"download.default_directory\": base_dir,\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "    }\n",
    ")\n",
    "options.headless = False \n",
    "options.add_argument('--proxy-server=%s' % PROXY)\n",
    "options.add_experimental_option('prefs', profile)\n",
    "options.add_argument('--kiosk-printing')\n",
    "path = \"C:\\\\Users\\\\Dishant\\\\Desktop\\\\chromedriver\\\\chromedriver\"\n",
    "\n",
    "global driver , speak , to_date_str\n",
    "# speak = wincl.Dispatch(\"SAPI.SpVoice\")    # for Speech \n",
    "driver = webdriver.Chrome(executable_path = path , options = options)\n",
    "global defendent_name_l , f_case_id , n_case_date , f_bank_name\n",
    "defendent_name_l=[]\n",
    "f_case_id=[]\n",
    "n_case_date=[]\n",
    "f_bank_name=[]\n",
    "\n",
    "\"\"\" All Categories of Foreclosures \"\"\"\n",
    "#RP & Foreclosure all 9 categories\n",
    "RP_F=['//*[@id=\"ctl00_Content1_caseCat\"]/option[17]','//*[@id=\"ctl00_Content1_caseCat\"]/option[18]',\n",
    "   '//*[@id=\"ctl00_Content1_caseCat\"]/option[32]','//*[@id=\"ctl00_Content1_caseCat\"]/option[19]',\n",
    "  '//*[@id=\"ctl00_Content1_caseCat\"]/option[33]','//*[@id=\"ctl00_Content1_caseCat\"]/option[34]',\n",
    "  '//*[@id=\"ctl00_Content1_caseCat\"]/option[43]','//*[@id=\"ctl00_Content1_caseCat\"]/option[44]',\n",
    "  '//*[@id=\"ctl00_Content1_caseCat\"]/option[45]']\n",
    "\n",
    "\"\"\"#to get the date for to date column#\"\"\"\n",
    "to_date_str = to_date()\n",
    "\n",
    "\"\"\" MAIN PROGRAM \"\"\"\n",
    "#speak.Speak(\"Jarvis Here .. Sir ... Starting The Program\")\n",
    "case_type=\"Civil Circuit\"\n",
    "\n",
    "for case_link_each in range(len(RP_F)):\n",
    "    login(RP_F[case_link_each])\n",
    "    time.sleep(5)\n",
    "    print(\"Total Number Of Cases Found = \",no_cases)\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    #Checking for number of cases\n",
    "    if(no_cases==0):\n",
    "        print(\"NO CASES FOUND\")\n",
    "    else:\n",
    "        data_collection_from_soup()\n",
    "\n",
    "        \n",
    "print(\"Data Collection Completed ... \")\n",
    "\n",
    "dataset = pd.DataFrame({\"CaseId\":f_case_id,\"CaseDate\":n_case_date,\"BankName\":f_bank_name,\"DefendantName\":defendent_name_l})\n",
    "dataset.index+=1\n",
    "dataset\n",
    "\n",
    "dataset.to_csv('All_Data_Volusia_County.csv')   # Final CSV File..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for gettting ClaimValue for each case details "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global case_num\n",
    "case_num=['2020 10420 CIDL', '2020 10436 CIDL', '2020 30478 CICI', '2020 30454 CICI', '2020 30405 CICI', '2020 30383 CICI', \n",
    "          '2020 30380 CICI', '2020 10463 CIDL', '2020 10438 CIDL', '2020 10432 CIDL', '2020 10405 CIDL', '2020 30482 CICI', \n",
    "          '2020 30445 CICI', '2020 30397 CICI', '2020 30381 CICI', '2020 30377 CICI', '2020 10448 CIDL', '2020 10433 CIDL', \n",
    "          '2020 10428 CIDL', '2020 10582 CIDL', '2020 30396 CICI', '2020 30434 CICI', '2020 30495 CICI', '2020 30411 CICI', \n",
    "          '2020 30416 CICI']\n",
    "\n",
    "\n",
    "\"\"\" TO LOGIN \"\"\"\n",
    "def login(x):\n",
    "    try:\n",
    "        \n",
    "        global no_cases\n",
    "        driver.get(\"https://app02.clerk.org/ccms\")\n",
    "        time.sleep(15)\n",
    "        driver.find_element_by_xpath(\"//*[@id='ctl00_Content1_button_accept']\").click()\n",
    "        time.sleep(10)\n",
    "        driver.find_element_by_xpath('//*[@id=\"ctl00_Content1_CaseNum\"]').send_keys(case_num[x])\n",
    "        time.sleep(7)\n",
    "        driver.find_element_by_id(\"ctl00_Content1_lb_submit\").click()\n",
    "        \n",
    "    except NoSuchElementException :\n",
    "        print(\"\\nError at login() = NoSuchElementException\")\n",
    "        print(\"*********BURNNING THE CODE AGAIN*********\")\n",
    "    except WebDriverException :\n",
    "        print(\"\\nError at login() = WebDriverException\")\n",
    "        print(\"*********BURN THE CODE AGAIN*********\")\n",
    "    except ElementClickInterceptedException :\n",
    "        print(\"\\nChange the Proxy IP ... Crawler Got Detected ...\")\n",
    "        print(\"Run the code Again...\")\n",
    "    except Exception as e:\n",
    "        print(\"\\nN-H  Exception = \",e)\n",
    "        print(\"*********BURN THE CODE AGAIN*********\")\n",
    "\n",
    "def pdf_claim_value():\n",
    "    filename = 'C:\\\\Users\\\\Dishant\\\\Downloads\\\\load_Redact.pdf'\n",
    "    pages = convert_from_path(filename, 500)\n",
    "    print(\"Number of Pages found in PDF = \",len(pages))\n",
    "    matches = \"\"\n",
    "    for page in pages:\n",
    "        page.save('C:\\\\Users\\\\Dishant\\\\Downloads\\\\property_value.jpg', 'JPEG')\n",
    "        property_value = str(((pytesseract.image_to_string(Image.open('C:\\\\Users\\\\Dishant\\\\Downloads\\\\property_value.jpg'))))).lower()\n",
    "        #print(property_value)\n",
    "        try:\n",
    "            matches = re.search(\"(\\\\d+\\\\W)*total estimated value of claim(\\\\W*\\\\d*)+\", property_value).group()\n",
    "            matches = matches.split(\"\\n\")\n",
    "            if \"total\" in matches[0]:\n",
    "                matches = matches [0]\n",
    "            else:\n",
    "                matches = matches[1]\n",
    "            #print(matches, \"1\")\n",
    "            claim_value = re.search(\"(\\\\d+\\\\W)+\", matches).group()\n",
    "            print(claim_value)\n",
    "        except Exception as e:\n",
    "            claim_value=\"0\"\n",
    "            print(\"pdf_claim_value() error = \",e)\n",
    "    # Appending claim_value to CVL list \n",
    "    if(claim_value==\"0\"):\n",
    "        CVL.append(\"0\")\n",
    "        print(\"ClaimValue = \",claim_value)\n",
    "    else:\n",
    "        CVL.append(claim_value)\n",
    "        print(\"ClaimValue = \",claim_value)\n",
    "    print(\"ITERATION  at pdf_claim_value() = \",CASE_NUM)\n",
    "    os.remove('C:\\\\Users\\\\Dishant\\\\Downloads\\\\property_value.jpg')\n",
    "    os.remove(filename)\n",
    "\n",
    "        \n",
    "###################################################################################################################\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from pdf2image import convert_from_path\n",
    "# import win32com.client as wincl       # Speech Module\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from PIL import Image \n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "import pyautogui\n",
    "import requests\n",
    "import random\n",
    "import PyPDF2 \n",
    "import json\n",
    "import mouse\n",
    "import time \n",
    "import os\n",
    "import re\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\\\Program Files (x86)\\\\Tesseract-OCR\\\\tesseract.exe\"\n",
    "base_dir = os.path.abspath(os.path.dirname(\"__file__\"))\n",
    "\n",
    "# Different Proxys : =    192.41.71.199:3128   50.246.120.125:8080      \n",
    "PROXY =   \"138.197.32.120:3128\"  #\"157.230.44.213:8080\"  #\"138.197.32.120:3128\"  #\"74.121.98.90:8080\"  \n",
    "            #\"192.144.215.244:3128\"  #\"157.230.44.213:8080\"   #\"74.121.98.90:8080\"    #\"192.41.71.221:3128\"\n",
    "options = webdriver.ChromeOptions()\n",
    "appState = {\n",
    "    \"recentDestinations\": [\n",
    "        {\n",
    "            \"id\": \"Save as PDF\",\n",
    "            \"origin\": \"local\",\n",
    "            \"account\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    \"selectedDestinationId\": \"Save as PDF\",\n",
    "    \"version\": 2\n",
    "}\n",
    "profile = {'printing.print_preview_sticky_settings.appState': json.dumps(appState)}\n",
    "options.add_experimental_option('prefs',  {\n",
    "    \"download.default_directory\": base_dir,\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "    }\n",
    ")\n",
    "options.headless = False #True  \n",
    "options.add_argument('--proxy-server=%s' % PROXY)\n",
    "options.add_experimental_option('prefs', profile)\n",
    "options.add_argument('--kiosk-printing')\n",
    "path = \"C:\\\\Users\\\\Dishant\\\\Desktop\\\\chromedriver\\\\chromedriver\"\n",
    "\n",
    "\"\"\" MAIN PROGRAM \"\"\"\n",
    "global driver , speak , claim_value , CVL\n",
    "CVL = []\n",
    "\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"All_Data_Volusia_County.csv\")\n",
    "global case_num\n",
    "case_num = []\n",
    "for i in dataset[\"CaseId\"]:\n",
    "    case_num.append(i)\n",
    "\n",
    "\n",
    "for CASE_NUM in range(len(case_num)): #\n",
    "    try:\n",
    "        driver = webdriver.Chrome(executable_path = path , options = options)\n",
    "        login(CASE_NUM)\n",
    "        time.sleep(15)\n",
    "        content = driver.page_source\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        \"\"\" This function helps to collect links from the page source \"\"\"\n",
    "        href_link = []\n",
    "        for row in soup.findAll('tr',attrs={'class':'rowstyle'}):\n",
    "            links = row.a['href']\n",
    "            href_link.append(\"https://app02.clerk.org/ccms/\"+links)\n",
    "        for row in soup.findAll('tr',attrs={'class':'altrowstyle'}):\n",
    "            links = row.a['href']\n",
    "            href_link.append(\"https://app02.clerk.org/ccms/\"+links)\n",
    "        print(\"Total Links Found = \",len(href_link))\n",
    "        time.sleep(5)\n",
    "        # To open the link we got in anew tab\n",
    "        a=\"window.open(\"+\"'\"+ href_link[0] +\"'\"+\")\"\n",
    "        print(a)\n",
    "        driver.execute_script(a)\n",
    "        time.sleep(8)\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        time.sleep(4)\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        time.sleep(4)\n",
    "        driver.find_element_by_xpath('//*[@id=\"__tab_docket\"]/span').click()\n",
    "        ######\n",
    "        data_row  =  []\n",
    "        table = driver.find_element_by_id(\"GridDocket\")\n",
    "        rows = table.find_elements_by_tag_name(\"tr\")\n",
    "\n",
    "        for index in range(2,len(rows)):\n",
    "            data_row.append(rows[index].text)\n",
    "\n",
    "        for i,j in enumerate(data_row):\n",
    "            try:\n",
    "                re.search(\"WOKRSHEET\",j).group()\n",
    "                print(\"index = \",i)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        javaScript = 'document.getElementsByClassName(\"button_imaged\")['+str((7+i)-2)+'].click();'\n",
    "        driver.execute_script(javaScript)\n",
    "        time.sleep(20)\n",
    "        driver.switch_to.window(driver.window_handles[2])\n",
    "        driver.execute_script('window.print();')\n",
    "        time.sleep(20)\n",
    "        os.rename(r'C:\\Users\\Dishant\\Downloads\\load_Redact.aspx.pdf',r'C:\\Users\\Dishant\\Downloads\\load_Redact.pdf')\n",
    "        time.sleep(6)\n",
    "        pdf_claim_value()\n",
    "        time.sleep(4)\n",
    "        driver.quit()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Error at main program = \",e)\n",
    "        driver.quit()\n",
    "        claim_value=\"0\"\n",
    "        CVL.append(claim_value)\n",
    "        print(\"ClaimValue = \",claim_value)\n",
    "        print(\"ITERATION at main exception = \",CASE_NUM)\n",
    "        \n",
    "#Creating new dataset and removing the old file\n",
    "        time.sleep(5)  \n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"All_Data_Volusia_County.csv\")\n",
    "dataset[\"RealValueString\"] = CVL\n",
    "os.remove(\"All_Data_Volusia_County.csv\")\n",
    "dataset.to_csv('All_Data_Volusia_County.csv')   # Final CSV File..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting all Address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" TO LOGIN \"\"\"\n",
    "def login(x):\n",
    "    try:\n",
    "        \n",
    "        global no_cases\n",
    "        driver.get(\"https://app02.clerk.org/ccms\")\n",
    "        time.sleep(15)\n",
    "        driver.find_element_by_xpath(\"//*[@id='ctl00_Content1_button_accept']\").click()\n",
    "        time.sleep(10)\n",
    "        driver.find_element_by_xpath('//*[@id=\"ctl00_Content1_CaseNum\"]').send_keys(case_num[x])\n",
    "        time.sleep(7)\n",
    "        driver.find_element_by_id(\"ctl00_Content1_lb_submit\").click()\n",
    "        \n",
    "    except NoSuchElementException :\n",
    "        print(\"\\nError at login() = NoSuchElementException\")\n",
    "        print(\"*********BURNNING THE CODE AGAIN*********\")\n",
    "    except WebDriverException :\n",
    "        print(\"\\nError at login() = WebDriverException\")\n",
    "        print(\"*********BURN THE CODE AGAIN*********\")\n",
    "    except ElementClickInterceptedException :\n",
    "        print(\"\\nChange the Proxy IP ... Crawler Got Detected ...\")\n",
    "        print(\"Run the code Again...\")\n",
    "    except Exception as e:\n",
    "        print(\"\\nN-H  Exception = \",e)\n",
    "        print(\"*********BURN THE CODE AGAIN*********\")\n",
    "\n",
    "def pdf_claim_value():\n",
    "    \n",
    "    filename = 'C:\\\\Users\\\\Dishant\\\\Downloads\\\\load_Redact.pdf'\n",
    "    pages = convert_from_path(filename, 500)\n",
    "    print(\"Number of Pages found in PDF = \",len(pages))\n",
    "    matches = \"\"\n",
    "    for page in pages:\n",
    "        page.save('C:\\\\Users\\\\Dishant\\\\Downloads\\\\property_value.jpg', 'JPEG')\n",
    "        property_value = str(((pytesseract.image_to_string(Image.open('C:\\\\Users\\\\Dishant\\\\Downloads\\\\property_value.jpg'))))).lower()\n",
    "        #print(property_value)\n",
    "        try:\n",
    "            matches = re.search(\"(\\\\d+\\\\W)*total estimated value of claim(\\\\W*\\\\d*)+\", property_value).group()\n",
    "            matches = matches.split(\"\\n\")\n",
    "            if \"total\" in matches[0]:\n",
    "                matches = matches [0]\n",
    "            else:\n",
    "                matches = matches[1]\n",
    "            #print(matches, \"1\")\n",
    "            claim_value = re.search(\"(\\\\d+\\\\W)+\", matches).group()\n",
    "            print(claim_value)\n",
    "            CVL.append(claim_value)\n",
    "            print(\"ITERATION at pdf_claim_value() try = \",CASE_NUM)\n",
    "        except Exception as e:\n",
    "            claim_value=\"0\"\n",
    "            CVL.append(claim_value)\n",
    "            time.sleep(2)\n",
    "            print(\"pdf_claim_value() error = \",e)\n",
    "            print(\"claim_value appended as zero\")\n",
    "            print(\"ITERATION  at pdf_claim_value() error = \",CASE_NUM)\n",
    "    os.remove('C:\\\\Users\\\\Dishant\\\\Downloads\\\\property_value.jpg')\n",
    "    os.remove(filename)\n",
    "\n",
    "\n",
    "\"\"\"CODE FOR PDF address extraction\"\"\"\n",
    "def PDFadd():\n",
    "    try: \n",
    "        filename = os.path.join(\"C:\\\\Users\\\\Dishant\\\\Downloads\\\\\", \"load_Redact.pdf\")        \n",
    "        pages = convert_from_path(filename, 500)\n",
    "        print(\"\\nNumber of Pages found in PDF = \",len(pages))\n",
    "        trials=0\n",
    "        start = \"\"\n",
    "        end = \"\"\n",
    "        add=\"\"\n",
    "        found = False\n",
    "        for page in pages:\n",
    "            add=\"\"\n",
    "            page.save('C:\\\\Users\\\\Dishant\\\\Downloads\\\\add_img.jpg', 'JPEG')\n",
    "            address_txt = str(((pytesseract.image_to_string(Image.open('C:\\\\Users\\\\Dishant\\\\Downloads\\\\add_img.jpg'))))).lower()\n",
    "            try:\n",
    "                add=\"\"\n",
    "                # If UNKNOWN is there \n",
    "                start = re.search(r'unknown',address_txt).span()\n",
    "                end = address_txt[start[1]:]\n",
    "                Unkonwn_Found = True\n",
    "                if(Unkonwn_Found):\n",
    "                    try:\n",
    "                        add=\"\"\n",
    "                        print(\"UNKNOWN FOUND\")\n",
    "                        ############ First Attempt\n",
    "                        try:\n",
    "                            add = re.search(\"\\d{2,5}\\s[\\b\\w,@\\s]*\\sfl\\s\\d{4,5}\\-\\d{3,5}\", end).group()\n",
    "                            print(add)\n",
    "                            print(\"Address_Found in Attempt = 1\")\n",
    "                            print(\"1\")\n",
    "                            comp_address.append(add)\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            add = re.search(r'\\d{2,5}\\s[\\b\\w,@\\s]*\\sfl\\s\\d{4,5}', end).group()\n",
    "                            print(add)\n",
    "                            print(\"Address_Found in (Exception)Attempt = 1\")\n",
    "                            print(\"2\")\n",
    "                            comp_address.append(add)\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        add=\"\"\n",
    "                        ############ Second Attempt\n",
    "                        try:\n",
    "                            add = re.search(\"\\d{2,5}\\s[\\b\\w,@\\s]*\\sflorida\\s\\d{4,5}\\-\\d{3,5}\", end).group()\n",
    "                            print(add)\n",
    "                            print(\"Address_Found in Attempt = 2\")\n",
    "                            print(\"3\")\n",
    "                            comp_address.append(add)\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            add = re.search(\"\\d{2,5}\\s[\\b\\w,@\\s]*\\sflorida\\s\\d{4,5}\", end).group()\n",
    "                            print(add)\n",
    "                            print(\"Address_Found in (Exception)Attempt = 2\")\n",
    "                            print(\"4\")\n",
    "                            comp_address.append(add)\n",
    "                            break\n",
    "\n",
    "            except Exception as e:\n",
    "                add=\"\"\n",
    "                try:\n",
    "                    add=\"\"\n",
    "                    print(\"UNKNOWN NOT FOUND\")\n",
    "                    ############ First Attempt\n",
    "                    try:\n",
    "                        add = re.search(\"\\d{2,5}\\s[\\b\\w,@\\s]*\\sfl\\s\\d{4,5}\\-\\d{3,5}\", address_txt).group()\n",
    "                        print(add)\n",
    "                        print(\"Address_Found in Attempt = 1\")\n",
    "                        print(\"5\")\n",
    "                        comp_address.append(add)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        add = re.search(r'\\d{2,5}\\s[\\b\\w,@\\s]*\\sfl\\s\\d{4,5}', address_txt).group()\n",
    "                        print(add)\n",
    "                        print(\"Address_Found in (Exception)Attempt = 1\")\n",
    "                        print(\"6\")\n",
    "                        comp_address.append(add)\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    add=\"\"\n",
    "                    ############ Second Attempt\n",
    "                    try:\n",
    "                        add = re.search(\"\\d{2,5}\\s[\\b\\w,@\\s]*\\sflorida\\s\\d{4,5}\\-\\d{3,5}\", address_txt).group()\n",
    "                        print(add)\n",
    "                        print(\"Address_Found in Attempt = 2\")\n",
    "                        print(\"7\")\n",
    "                        comp_address.append(add)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        add = re.search(\"\\d{2,5}\\s[\\b\\w,@\\s]*\\sflorida\\s\\d{4,5}\", address_txt).group()\n",
    "                        print(add)\n",
    "                        print(\"Address_Found in (Exception)Attempt = 2\")\n",
    "                        print(\"8\")\n",
    "                        comp_address.append(add)\n",
    "                        break\n",
    "    except Exception as e:\n",
    "        print(\"ERROR\")\n",
    "        # If add is empty .....  \n",
    "    if(add==\"\"):\n",
    "        comp_address.append(\"NAF\")\n",
    "    os.remove('C:\\\\Users\\\\Dishant\\\\Downloads\\\\add_img.jpg')\n",
    "    os.remove(filename)\n",
    "    print(\"ITERATION at PDFadd() exception = \",CASE_NUM)\n",
    "    #os.close(filename)\n",
    "\n",
    "    \n",
    "###################################################################################################################\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from pdf2image import convert_from_path\n",
    "# import win32com.client as wincl       # Speech Module\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from PIL import Image \n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "import pyautogui\n",
    "import requests\n",
    "import random\n",
    "import PyPDF2 \n",
    "import json\n",
    "import mouse\n",
    "import time \n",
    "import os\n",
    "import re\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\\\Program Files (x86)\\\\Tesseract-OCR\\\\tesseract.exe\"\n",
    "base_dir = os.path.abspath(os.path.dirname(\"__file__\"))\n",
    "\n",
    "# Different Proxys : =    192.41.71.199:3128   50.246.120.125:8080      \n",
    "PROXY =   \"138.197.32.120:3128\"  #\"157.230.44.213:8080\"  #\"138.197.32.120:3128\"  #\"74.121.98.90:8080\"  \n",
    "            #\"192.144.215.244:3128\"  #\"157.230.44.213:8080\"   #\"74.121.98.90:8080\"    #\"192.41.71.221:3128\"\n",
    "options = webdriver.ChromeOptions()\n",
    "appState = {\n",
    "    \"recentDestinations\": [\n",
    "        {\n",
    "            \"id\": \"Save as PDF\",\n",
    "            \"origin\": \"local\",\n",
    "            \"account\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    \"selectedDestinationId\": \"Save as PDF\",\n",
    "    \"version\": 2\n",
    "}\n",
    "profile = {'printing.print_preview_sticky_settings.appState': json.dumps(appState)}\n",
    "options.add_experimental_option('prefs',  {\n",
    "    \"download.default_directory\": base_dir,\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "    }\n",
    ")\n",
    "options.headless = False #True  \n",
    "options.add_argument('--proxy-server=%s' % PROXY)\n",
    "options.add_experimental_option('prefs', profile)\n",
    "options.add_argument('--kiosk-printing')\n",
    "path = \"C:\\\\Users\\\\Dishant\\\\Desktop\\\\chromedriver\\\\chromedriver\"\n",
    "\n",
    "\"\"\" MAIN PROGRAM \"\"\"\n",
    "global driver , comp_address\n",
    "comp_address = []\n",
    "##Collecting CaseId from csv file\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"All_Data_Volusia_County.csv\")\n",
    "global case_num\n",
    "case_num = []\n",
    "for i in dataset[\"CaseId\"]:\n",
    "    case_num.append(i)\n",
    "##Starting for loo[]\n",
    "for CASE_NUM in range(len(case_num)): #\n",
    "    try:\n",
    "        driver = webdriver.Chrome(executable_path = path , options = options)\n",
    "        login(CASE_NUM)\n",
    "        time.sleep(15)\n",
    "        content = driver.page_source\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        \"\"\" This function helps to collect links from the page source \"\"\"\n",
    "        href_link = []\n",
    "        for row in soup.findAll('tr',attrs={'class':'rowstyle'}):\n",
    "            links = row.a['href']\n",
    "            href_link.append(\"https://app02.clerk.org/ccms/\"+links)\n",
    "        for row in soup.findAll('tr',attrs={'class':'altrowstyle'}):\n",
    "            links = row.a['href']\n",
    "            href_link.append(\"https://app02.clerk.org/ccms/\"+links)\n",
    "        print(\"Total Links Found = \",len(href_link))\n",
    "        time.sleep(5)\n",
    "        # To open the link we got in anew tab\n",
    "        a=\"window.open(\"+\"'\"+ href_link[0] +\"'\"+\")\"\n",
    "        print(a)\n",
    "        driver.execute_script(a)\n",
    "        time.sleep(8)\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        time.sleep(4)\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        time.sleep(4)\n",
    "        driver.find_element_by_xpath('//*[@id=\"__tab_docket\"]/span').click()\n",
    "        ######\n",
    "        data_row  =  []\n",
    "        table = driver.find_element_by_id(\"GridDocket\")\n",
    "        rows = table.find_elements_by_tag_name(\"tr\")\n",
    "\n",
    "        for index in range(2,len(rows)):\n",
    "            data_row.append(rows[index].text)\n",
    "\n",
    "        for i,j in enumerate(data_row):\n",
    "            try:\n",
    "                re.search(\"REQUEST\\sSUMMONS\",j).group()\n",
    "                print(\"index = \",i)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        javaScript = 'document.getElementsByClassName(\"button_imaged\")['+str((7+i))+'].click();'\n",
    "        driver.execute_script(javaScript)\n",
    "        time.sleep(20)\n",
    "        driver.switch_to.window(driver.window_handles[2])\n",
    "        driver.execute_script('window.print();')\n",
    "        time.sleep(20)\n",
    "        os.rename(r'C:\\Users\\Dishant\\Downloads\\load_Redact.aspx.pdf',r'C:\\Users\\Dishant\\Downloads\\load_Redact.pdf')\n",
    "        time.sleep(6)\n",
    "        PDFadd()\n",
    "        time.sleep(4)\n",
    "        driver.quit()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Error at main program = \",e)\n",
    "        driver.quit()\n",
    "        comp_address.append(\"NAF\")\n",
    "        print(\"comp_address appended as NAF\")\n",
    "        print(\"ITERATION at main exception = \",CASE_NUM)\n",
    "\n",
    "########### Cleaning the Gathered Address         \n",
    "demo_list = []\n",
    "for i in comp_address:\n",
    "    if(i==\"NAF\"):\n",
    "        demo_list.append(\"0\")\n",
    "    else:\n",
    "        demo_list.append(i)\n",
    "len(demo_list)\n",
    "\"\"\"@@@@@\"\"\"\n",
    "# Updating the file and doing some changes\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"All_Data_Volusia_County.csv\")\n",
    "dataset[\"Comp_Address\"] = demo_list\n",
    "### if extra columns are seen in dataset after adding new coloumns then do this step\n",
    "# del dataset[\"Unnamed: 0\"]\n",
    "# del dataset[\"Unnamed: 0.1\"]\n",
    "os.remove(\"All_Data_Volusia_County.csv\")\n",
    "dataset.to_csv('All_Data_Volusia_County.csv')   # Final CSV File..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing Calculations to Create Final Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"FOR GOING TO WESITE FOR PROPERTY VALUE\"\"\"\n",
    "def prop_val(x,y,z):\n",
    "    try:\n",
    "        driver.get(\"http://www.totalviewrealestate.com/\")  \n",
    "        time.sleep(10)   \n",
    "        driver.find_element_by_xpath('//*[@id=\"header\"]/form/p/input[1]').send_keys(x) \n",
    "        time.sleep(2)\n",
    "        driver.find_element_by_xpath('//*[@id=\"header\"]/form/p/input[2]').send_keys(y) \n",
    "        time.sleep(2)\n",
    "        driver.find_element_by_xpath('//*[@id=\"header\"]/form/p/input[3]').send_keys(z) \n",
    "        time.sleep(4)\n",
    "        driver.find_element_by_xpath('//*[@id=\"header\"]/form/input').click() #SUBMIT\n",
    "        time.sleep(17)\n",
    "        data = driver.find_element_by_xpath('//*[@id=\"leftpi\"]/p[2]').text\n",
    "        print(data)\n",
    "        lis.append(data)\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        print(\"Error at pop_val() = \",e)\n",
    "        lis.append(\"NAF\")\n",
    "\n",
    "\n",
    "\"\"\"FOR CLAIM VALUE CONVERTING TO INTEGER\"\"\"\n",
    "def claim_val_converter(string_value):\n",
    "    C_Value = \"\"\n",
    "    try:\n",
    "        for i in string_value.split(\".\")[0]: \n",
    "            if(i.isdigit()):\n",
    "\n",
    "                C_Value = C_Value + i\n",
    "        return int(C_Value)\n",
    "    except Exception as e:\n",
    "        C_Value = \"NA\"\n",
    "        return C_Value\n",
    "\n",
    "\n",
    "\"\"\"FOR HIGH VALUATION VALUE\"\"\"\n",
    "def HVVF():\n",
    "    high_value = \"\"\n",
    "    for i in HVS: \n",
    "        if(i.isdigit()):\n",
    "            high_value = high_value + i\n",
    "    return high_value\n",
    "\n",
    "\"\"\"FOR LOW VALUATION VALUE\"\"\"\n",
    "def LVVF():\n",
    "    low_value = \"\"\n",
    "    for i in LVS: \n",
    "        if(i.isdigit()):\n",
    "            low_value = low_value + i\n",
    "    return low_value\n",
    "         \n",
    "    \n",
    "#######################################################\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from pdf2image import convert_from_path\n",
    "# import win32com.client as wincl       # Speech Module\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from PIL import Image \n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "import pyautogui\n",
    "import requests\n",
    "import random\n",
    "import PyPDF2 \n",
    "import json\n",
    "import mouse\n",
    "import time \n",
    "import os\n",
    "import re\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\\\Program Files (x86)\\\\Tesseract-OCR\\\\tesseract.exe\"\n",
    "base_dir = os.path.abspath(os.path.dirname(\"__file__\"))\n",
    "\n",
    "# Different Proxys : =    192.41.71.199:3128   50.246.120.125:8080      \n",
    "PROXY =   \"138.197.32.120:3128\"  #\"157.230.44.213:8080\"  #\"138.197.32.120:3128\"  #\"74.121.98.90:8080\"  \n",
    "            #\"192.144.215.244:3128\"  #\"157.230.44.213:8080\"   #\"74.121.98.90:8080\"    #\"192.41.71.221:3128\"\n",
    "options = webdriver.ChromeOptions()\n",
    "appState = {\n",
    "    \"recentDestinations\": [\n",
    "        {\n",
    "            \"id\": \"Save as PDF\",\n",
    "            \"origin\": \"local\",\n",
    "            \"account\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    \"selectedDestinationId\": \"Save as PDF\",\n",
    "    \"version\": 2\n",
    "}\n",
    "profile = {'printing.print_preview_sticky_settings.appState': json.dumps(appState)}\n",
    "options.add_experimental_option('prefs',  {\n",
    "    \"download.default_directory\": base_dir,\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "    }\n",
    ")\n",
    "options.headless = False #True  \n",
    "options.add_argument('--proxy-server=%s' % PROXY)\n",
    "options.add_experimental_option('prefs', profile)\n",
    "options.add_argument('--kiosk-printing')\n",
    "path = \"C:\\\\Users\\\\Dishant\\\\Desktop\\\\chromedriver\\\\chromedriver\"\n",
    "\n",
    "global driver , lis\n",
    "lis = []\n",
    "\n",
    "driver = webdriver.Chrome(executable_path = path , options = options)\n",
    "\n",
    "\"\"\"MAIN PROGRAM\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"All_Data_Volusia_County.csv\")\n",
    "\n",
    "\"\"\"FOR GETTING CLAIM VALUE FROM DATASET\"\"\"\n",
    "got_str_values = dataset[\"RealValueString\"]\n",
    "new_int_real_value = []\n",
    "for i in range(len(got_str_values)):\n",
    "    got_int_value = claim_val_converter(got_str_values[i])\n",
    "    new_int_real_value.append(got_int_value)\n",
    "    #print(got_int_value)\n",
    "# Inserting integer Real_Value into dataset\n",
    "dataset[\"RealValue\"] = new_int_real_value\n",
    "del dataset[\"Unnamed: 0\"]\n",
    "\n",
    "\n",
    "new = []\n",
    "for i in dataset[\"Comp_Address\"]:\n",
    "    new.append(i)\n",
    "print(len(new))\n",
    "new \n",
    "######\n",
    "for i in range(len(new)):\n",
    "#     print(new[i])\n",
    "    if(new[i]==\"0\"):\n",
    "        lis.append(\"NAF\")\n",
    "        print(\"NAF\",\"iteration  = \",i)\n",
    "        continue\n",
    "    try:\n",
    "        try:\n",
    "            try:\n",
    "                xyz = re.search(\"\\n\",new[i]).group()\n",
    "                demo = new[i].split(\"\\n\")\n",
    "                Address = demo[0]\n",
    "                City = demo[1].split(\",\")[0]\n",
    "                pin =  demo[1].split(\",\")[1].split(\" \")[2]\n",
    "                prop_val(Address,City,pin)\n",
    "                print(Address,\"|\",City,\"|\",pin,\"iteration  = \",i)\n",
    "            except Exception as e:\n",
    "                demo = new[i].split(\",\")\n",
    "                Address = demo[0]\n",
    "                City = demo[1]\n",
    "                pin =  demo[2].split(\" \")[2]\n",
    "                prop_val(Address,City,pin)\n",
    "                print(Address,\"|\",City,\"|\",pin,\"iteration  = \",i)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                re.search(\"\\n\\n\",new[i]).group()\n",
    "                demo = new[i].split(\"\\n\")\n",
    "                Address = demo[0]\n",
    "                City = demo[2].split(\",\")[0]\n",
    "                pin =  demo[2].split(\",\")[1].split(\" \")[2]\n",
    "                prop_val(Address,City,pin)\n",
    "                print(Address,\"|\",City,\"|\",pin,\"iteration  = \",i)\n",
    "            except Exception as e:\n",
    "                demo = new[i].split(\",\")\n",
    "                Address = demo[0]\n",
    "                City = demo[1]\n",
    "                pin =  demo[2].split(\" \")[2]\n",
    "                prop_val(Address,City,pin)\n",
    "                print(Address,\"|\",City,\"|\",pin,\"iteration  = \",i)\n",
    "    except Exception as e:\n",
    "        print(\"Error = \",e,\"  iteration  = \",i)\n",
    "        lis.append(\"NAF\")\n",
    "        continue            \n",
    "driver.quit()\n",
    "    \n",
    "#Cleaning the gathered data into four lists for Calculations      \n",
    "list_HVS = []\n",
    "list_HVV = []\n",
    "list_LVS = []\n",
    "list_LVV = []\n",
    "\n",
    "\"\"\" FOR EXTRACTING HIGH LOW VALUE OF PROPRERTY \"\"\"\n",
    "for i in range(len(lis)):\n",
    "    demo = lis[i]\n",
    "    try:\n",
    "        #print(demo)\n",
    "        start = re.search(\"Value\\sRange\",demo).span()\n",
    "        #print(start)\n",
    "        end = re.search(\"Change\\:\\s\",demo).span()\n",
    "        #print(end)\n",
    "        got = demo[start[1]:end[0]]\n",
    "        got.split(\"to\")\n",
    "        HVS = got.split(\"to\")[1].split(\"\\n\")[0]\n",
    "        HVV = HVVF()\n",
    "        LVS = got.split(\"to\")[0].split(\" \")[1]\n",
    "        LVV = LVVF()\n",
    "        print(\"\\n\")\n",
    "        print(\"Low Value String = \",LVS)\n",
    "        print(\"Low Value = \",LVV)\n",
    "        print(\"High Value String = \",HVS)\n",
    "        print(\"High Value = \",HVV)\n",
    "        list_HVS.append(HVS)\n",
    "        list_HVV.append(int(HVV))\n",
    "        list_LVS.append(LVS)\n",
    "        list_LVV.append(int(LVV))\n",
    "    except Exception as e:\n",
    "        HVS = \"NA\"\n",
    "        HVV = \"NA\"\n",
    "        LVS = \"NA\"\n",
    "        LVV = \"NA\"\n",
    "        print(\"\\n\")\n",
    "        print(\"Low Value String = \",LVS)\n",
    "        print(\"Low Value = \",LVV)\n",
    "        print(\"High Value String = \",HVS)\n",
    "        print(\"High Value = \",HVV)\n",
    "        list_HVS.append(HVS)\n",
    "        list_HVV.append(HVV)\n",
    "        list_LVS.append(LVS)\n",
    "        list_LVV.append(LVV)\n",
    "        \n",
    "\n",
    "\"\"\"FOR CALCULATIONS\"\"\"\n",
    "Avg_App_V_L=[]\n",
    "Avg_Est_V_L = []\n",
    "Acq_Ratio_L = []\n",
    "\n",
    "\n",
    "for i in range(len(new_int_real_value)):\n",
    "    try:\n",
    "        real_value = new_int_real_value[i]\n",
    "        high = list_HVV[i]\n",
    "        #print(high)\n",
    "        low = list_LVV[i]\n",
    "        #print(low)\n",
    "\n",
    "        if(high==\"NA\" or low == \"NA\"):\n",
    "            Avg_App_V = \"NA\"   \n",
    "            Avg_App_V_L.append(Avg_App_V) #Average Approximate Value\n",
    "            Avg_Est_V = \"NA\" \n",
    "            Avg_Est_V_L.append(Avg_Est_V) #Average Estimated Value\n",
    "            Acq_Ratio = \"NA\"\n",
    "            Acq_Ratio_L.append(Acq_Ratio) #Acquisation Ratio\n",
    "        else:\n",
    "            Avg_App_V = (high+low)/2   \n",
    "            Avg_App_V_L.append(Avg_App_V) #Average Approximate Value\n",
    "            if(real_value == \"NA\"):\n",
    "                Avg_Est_V = \"NA\"\n",
    "                Avg_Est_V_L.append(Avg_Est_V) #Average Estimated Value\n",
    "                Acq_Ratio = \"NA\"\n",
    "                Acq_Ratio_L.append(Acq_Ratio) #Acquisation Ratio\n",
    "\n",
    "            else:\n",
    "                Avg_Est_V = Avg_App_V - real_value \n",
    "                Avg_Est_V_L.append(Avg_Est_V) #Average Estimated Value\n",
    "                Acq_Ratio = (real_value / Avg_App_V) * 100\n",
    "                Acq_Ratio_L.append(Acq_Ratio) #Acquisation Ratio\n",
    "    except Exception as e:\n",
    "        Avg_App_V = \"NA\"   \n",
    "        Avg_App_V_L.append(Avg_App_V) #Average Approximate Value\n",
    "        Avg_Est_V = \"NA\" \n",
    "        Avg_Est_V_L.append(Avg_Est_V) #Average Estimated Value\n",
    "        Acq_Ratio = \"NA\"\n",
    "        Acq_Ratio_L.append(Acq_Ratio) #Acquisation Ratio\n",
    "        \n",
    "\"\"\" Getting Address , City , Pincode \"\"\"\n",
    "new = []\n",
    "for i in dataset[\"Comp_Address\"]:\n",
    "    new.append(i)\n",
    "# print(len(new))\n",
    "# new \n",
    "\n",
    "\"\"\"ADDING AQUIRED DATA TO DATAFRAME\"\"\"\n",
    "address = []\n",
    "city = []\n",
    "zipcode  = []\n",
    "\n",
    "for i in range(len(new)):\n",
    "    if(new[i]==\"0\"):\n",
    "        address.append(\"0\")\n",
    "        city.append(\"0\")\n",
    "        zipcode.append(\"0\")\n",
    "        continue\n",
    "    try:\n",
    "        try:\n",
    "            try:\n",
    "                xyz = re.search(\"\\n\",new[i]).group()\n",
    "                demo = new[i].split(\"\\n\")\n",
    "                Address = demo[0]\n",
    "                City = demo[1].split(\",\")[0]\n",
    "                pin =  demo[1].split(\",\")[1].split(\" \")[2]\n",
    "                address.append(Address)\n",
    "                city.append(City)\n",
    "                zipcode.append(\"FL \" + pin)\n",
    "                print(Address,\"|\",City,\"|\",pin,\"iteration  = \",i)\n",
    "            except Exception as e:\n",
    "                demo = new[i].split(\",\")\n",
    "                Address = demo[0]\n",
    "                City = demo[1]\n",
    "                pin =  demo[2].split(\" \")[2]\n",
    "                address.append(Address)\n",
    "                city.append(City)\n",
    "                zipcode.append(\"FL \" + pin)\n",
    "                print(Address,\"|\",City,\"|\",pin,\"iteration  = \",i)        \n",
    "        except Exception as e:\n",
    "            try:\n",
    "                re.search(\"\\n\\n\",new[i]).group()\n",
    "                demo = new[i].split(\"\\n\")\n",
    "                Address = demo[0]\n",
    "                City = demo[2].split(\",\")[0]\n",
    "                pin =  demo[2].split(\",\")[1].split(\" \")[2]\n",
    "                address.append(Address)\n",
    "                city.append(City)\n",
    "                zipcode.append(\"FL \" + pin)\n",
    "                print(Address,\"|\",City,\"|\",pin,\"iteration  = \",i)            \n",
    "            except Exception as e:\n",
    "                demo = new[i].split(\",\")\n",
    "                Address = demo[0]\n",
    "                City = demo[1]\n",
    "                pin =  demo[2].split(\" \")[2]\n",
    "                address.append(Address)\n",
    "                city.append(City)\n",
    "                zipcode.append(\"FL \" + pin)\n",
    "                print(Address,\"|\",City,\"|\",pin,\"iteration  = \",i)    \n",
    "    except Exception as e:\n",
    "        print(\"Error = \",e,\"  iteration  = \",i)\n",
    "        address.append(\"0\")\n",
    "        city.append(\"0\")\n",
    "        zipcode.append(\"0\")\n",
    "        continue\n",
    "\"\"\"FINAL CLEANING OF DATA\"\"\"\n",
    "county_name = [] \n",
    "case_type = [] \n",
    "RV = []\n",
    "for x in range(len(lis)): county_name.append(\"Volusia\")\n",
    "for i in range(len(lis)): case_type.append(\"Civil Circuit\")\n",
    "dataset.insert(loc=2, column='CaseType', value = case_type)\n",
    "for i in dataset[\"RealValue\"]: RV.append(i)\n",
    "del dataset[\"RealValue\"]\n",
    "dataset.insert(loc=6, column='RealValue', value = RV) #Add RealValue\n",
    "del dataset[\"Comp_Address\"]\n",
    "dataset['Address'] = address\n",
    "dataset['City'] = city\n",
    "dataset['Zipcode'] = zipcode\n",
    "dataset['HighValuationString_'] = list_HVS\n",
    "dataset['HighValuationValue_'] = list_HVV\n",
    "dataset['LowValuationString_'] = list_LVS\n",
    "dataset['LowValuationValue_'] = list_LVV\n",
    "dataset['AverageApproximateValue_'] = Avg_App_V_L\n",
    "dataset['AverageEstimateValue_'] = Avg_Est_V_L\n",
    "dataset['AcquisitionRatio_'] = Acq_Ratio_L\n",
    "\n",
    "#########\n",
    "RVS = []\n",
    "for i in dataset[\"RealValueString\"]:\n",
    "    if(i==\"NVF\"):\n",
    "        RVS.append(\"0\")\n",
    "    else:\n",
    "        RVS.append(i)\n",
    "del dataset[\"RealValueString\"]\n",
    "dataset.insert(loc=5, column='RealValueString', value = RVS) #Add RealValue\n",
    "\n",
    "#######\n",
    "RVN = []\n",
    "for i in dataset[\"RealValue\"]:\n",
    "    if(i==\"NA\"):\n",
    "        RVN.append(\"0\")\n",
    "    else:\n",
    "        RVN.append(i)\n",
    "del dataset[\"RealValue\"]\n",
    "dataset.insert(loc=6, column='RealValue', value = RVN) \n",
    "\n",
    "#########\n",
    "H = []\n",
    "for i in dataset[\"HighValuationString_\"]:\n",
    "    if(i==\"NA\"):\n",
    "        H.append(\"0\")\n",
    "    else:\n",
    "        H.append(i)\n",
    "dataset.insert(loc=10, column='HighValuationString', value = H) \n",
    "del dataset[\"HighValuationString_\"]\n",
    "#####\n",
    "H = []\n",
    "for i in dataset[\"HighValuationValue_\"]:\n",
    "    if(i==\"NA\"):\n",
    "        H.append(\"0\")\n",
    "    else:\n",
    "        H.append(i)\n",
    "dataset.insert(loc=11, column='HighValuationValue', value = H) \n",
    "del dataset[\"HighValuationValue_\"]\n",
    "#####\n",
    "H = []\n",
    "for i in dataset[\"LowValuationString_\"]:\n",
    "    if(i==\"NA\"):\n",
    "        H.append(\"0\")\n",
    "    else:\n",
    "        H.append(i)\n",
    "dataset.insert(loc=12, column='LowValuationString', value = H) \n",
    "del dataset[\"LowValuationString_\"]\n",
    "#####\n",
    "H = []\n",
    "for i in dataset[\"LowValuationValue_\"]:\n",
    "    if(i==\"NA\"):\n",
    "        H.append(\"0\")\n",
    "    else:\n",
    "        H.append(i)\n",
    "dataset.insert(loc=13, column='LowValuationValue', value = H) \n",
    "del dataset[\"LowValuationValue_\"]\n",
    "#####\n",
    "H = []\n",
    "for i in dataset[\"AverageApproximateValue_\"]:\n",
    "    if(i==\"NA\"):\n",
    "        H.append(\"0\")\n",
    "    else:\n",
    "        H.append(i)\n",
    "dataset.insert(loc=14, column='AverageApproximateValue', value = H) \n",
    "del dataset[\"AverageApproximateValue_\"]\n",
    "########\n",
    "H = []\n",
    "for i in dataset[\"AverageEstimateValue_\"]:\n",
    "    if(i==\"NA\"):\n",
    "        H.append(\"0\")\n",
    "    else:\n",
    "        H.append(i)\n",
    "dataset.insert(loc=15, column='AverageEstimateValue', value = H) \n",
    "del dataset[\"AverageEstimateValue_\"]\n",
    "#######33\n",
    "H = []\n",
    "for i in dataset[\"AcquisitionRatio_\"]:\n",
    "    if(i==\"NA\"):\n",
    "        H.append(\"0\")\n",
    "    else:\n",
    "        H.append(i)\n",
    "dataset.insert(loc=16, column='AcquisitionRatio', value = H) \n",
    "del dataset[\"AcquisitionRatio_\"]\n",
    "####\n",
    "dataset['CountyName'] = county_name\n",
    "\n",
    "\"\"\"DATE CHANGE TO DD/MM/YYYY FORMAT\"\"\"\n",
    "ncd=[]\n",
    "for i in dataset[\"CaseDate\"]:\n",
    "    try:\n",
    "        n_x = i.split(\"-\")\n",
    "        date = n_x[0]+\"/\"+n_x[1]+\"/\"+n_x[2]\n",
    "        ncd.append(date)\n",
    "    except Exception as e:\n",
    "        print(\"Error at Date Change\\t\",e)\n",
    "        ncd.append(\"0\")\n",
    "del dataset[\"CaseDate\"]\n",
    "dataset.insert(loc=1, column='CaseDate', value=ncd) #Add casedate dd/mm/yy\n",
    "os.remove(\"All_Data_Volusia_County.csv\")\n",
    "dataset.to_csv('All_Data_Volusia_County.csv')   # Final CSV File...\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len( lis ))\n",
    "print(len( address ))\n",
    "print(len( city ))\n",
    "print(len( zipcode ))\n",
    "print(len( Avg_App_V_L ))\n",
    "print(len( Avg_Est_V_L ))\n",
    "print(len( Acq_Ratio_L ))\n",
    "print(len( list_HVS ))\n",
    "print(len( list_HVV ))\n",
    "print(len( list_LVS ))\n",
    "print(len( list_LVV ))\n",
    "print(len( new ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TO GET CURRENT FOLDER PATH \"\"\"\n",
    "import os\n",
    "base_dir = os.path.abspath(os.path.dirname(\"__file__\"))\n",
    "PROJECT_ROOT = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "BASE_DIR = os.path.dirname(\"__file__\")\n",
    "print(base_dir)\n",
    "print(PROJECT_ROOT)\n",
    "# print(BASE_DIR)\n",
    "print(os.path.realpath(\"__file__\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.w3.org/')\n",
    "for a in driver.find_elements_by_xpath('.//a'):\n",
    "    print(a.get_attribute('href'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
