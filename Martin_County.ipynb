{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting all details Except Claim Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def login():\n",
    "    \"\"\" LOGIN INTO THE WEBSITE WITH ALL DETAILS \"\"\"\n",
    "    try:\n",
    "        driver.get(\"https://court.martinclerk.com\")\n",
    "        #To load Website Completely\n",
    "        time.sleep(3)\n",
    "        #To enter the dates from and to\n",
    "        DateFrom = driver.find_element_by_id('openedFrom')\n",
    "        DateFrom.click()\n",
    "        DateFrom.send_keys(\"03162020\")\n",
    "\n",
    "        DateTo = driver.find_element_by_id('openedTo')\n",
    "        DateTo.click()\n",
    "        DateTo.send_keys(\"03222020\")\n",
    "\n",
    "        driver.find_element_by_xpath(\"//*[@id='courTypesButton']\").click()\n",
    "        driver.find_element_by_xpath(\"//*[@id='courTypesMultiSelect']/ul/li[1]/a/label/input\").click()\n",
    "        time.sleep(2) \n",
    "        driver.find_element_by_xpath(\"//*[@id='courTypesMultiSelect']/ul/li[3]/a/label/input\").click()\n",
    "\n",
    "        #To Get the next DropDownList\n",
    "        time.sleep(10)\n",
    "        driver.find_element_by_xpath(\"//*[@id='courTypesButton']\").click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        driver.find_element_by_xpath(\"//*[@id='caseTypesButton']\").click()\n",
    "        driver.find_element_by_xpath(\"//*[@id='caseTypesMultiSelect']/ul/li[1]/a/label/input\").click()\n",
    "        time.sleep(1)\n",
    "        #To select the Categories\n",
    "        for i in range(69,78):\n",
    "            driver.find_element_by_xpath(\"//*[@id='caseTypesMultiSelect']/ul/li[\"+str(i)+\"]/a/label/input\").click()\n",
    "\n",
    "        Captcha_Entry_Box = driver.find_element_by_xpath(\"//*[@id='mainTableContent']/tbody/tr/td/table/tbody/tr[2]/td[2]/div/div[3]/form/input[2]\")\n",
    "        Captcha_Entry_Box.click()\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        captcha_additon = int(input(\"Enter the additon check the headless browser :\\t\"))\n",
    "        Captcha_Entry_Box.send_keys(str(captcha_additon))\n",
    "\n",
    "        #To click on Search Button\n",
    "        Search_Button = driver.find_element_by_xpath(\"//*[@id='searchButton']\")\n",
    "        Search_Button.click()\n",
    "\n",
    "        #To Load all the classes on webpage\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        print(\"Error at Login() = \",e)\n",
    "        print(\"BURN THE CODE AGAIN....!!\")\n",
    "\n",
    "def collect_data_from_search():\n",
    "    \"\"\" COLLECT DATA FROM SEARCH RESULT TO USE FOR ITERATIONS \"\"\"\n",
    "    try:\n",
    "        table = driver.find_element_by_id(\"gridSearchResults\")\n",
    "        rows = table.find_elements_by_tag_name(\"tr\")\n",
    "\n",
    "        global data_row\n",
    "        data_row = []\n",
    "        for index in range(1,len(rows)):\n",
    "            data_row.append(rows[index].text)\n",
    "        print(\"Lenght of Data Collected From Rows = \",len(data_row))\n",
    "    except Exception as e:\n",
    "        print(\"Error at collect_data_from_search() = \",e)\n",
    "\n",
    "\n",
    "def Clean_raw_data():\n",
    "    \"\"\" CLEANNING RAW DATA \"\"\"\n",
    "    global case_id , bank_name , def_name \n",
    "    try:\n",
    "        #Cleanning the raw_data ...... !!!\n",
    "        case_id = []\n",
    "        bank_name = []\n",
    "        def_name = []\n",
    "        for i in range(len(raw_data)):\n",
    "            caseid = raw_data[i].split(\"-\")[0]\n",
    "            case_id.append(caseid)\n",
    "            bankname = raw_data[i].split(\"-\")[1].split(\"vs.\")[0]\n",
    "            bank_name.append(bankname)\n",
    "            defname = raw_data[i].split(\"-\")[1].split(\"vs.\")[1]\n",
    "            def_name.append(defname)\n",
    "\n",
    "        print(len(bank_name),len(case_id),len(def_name))\n",
    "    except Exception as e:\n",
    "        print(\"Error at Clean_raw_data() = \",e)\n",
    "\n",
    "        \n",
    "def PDF_download():\n",
    "    \"\"\" DOWNLOADING THE PDF \"\"\"\n",
    "    try:\n",
    "        #Searching For PDF\n",
    "        table = driver.find_element_by_id(\"gridDockets\")\n",
    "        rows = table.find_elements_by_tag_name(\"tr\")\n",
    "\n",
    "        #Gathering Data From GRID_DOCKETS {PDF search}\n",
    "        gridDockets = []\n",
    "        for i in range(1,len(rows)):\n",
    "            gridDockets.append(rows[i].text)\n",
    "        print(\"Lenght of Data Collected From Rows = \",len(gridDockets))\n",
    "        #Searching for PDF present or not\n",
    "        for index,data in enumerate(gridDockets):\n",
    "            if ( \"VALUE OF REAL PROPERTY OR MORTGAGE FORECLOSURE CLAIM:\" in data ):\n",
    "                print(\"PDF Found index = \",index)\n",
    "                #Clicking on the PDF imaged button\n",
    "                driver.find_element_by_xpath('//*[@id=\"28896399\"]').click()\n",
    "                time.sleep(3)\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "                time.sleep(3)\n",
    "                driver.switch_to.window(driver.window_handles[1])\n",
    "                time.sleep(10)\n",
    "                driver.execute_script('window.print();')\n",
    "            else:\n",
    "                pass\n",
    "    except Exception as e:\n",
    "        print(\"Error at PDF_download() = \",e)\n",
    "\n",
    "\n",
    "\"\"\"###############################################\"\"\"\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from pdf2image import convert_from_path\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from PIL import Image \n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "import pyautogui\n",
    "import requests\n",
    "import random\n",
    "import PyPDF2 \n",
    "import json\n",
    "import mouse\n",
    "import time \n",
    "import os\n",
    "import re\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\\\Program Files (x86)\\\\Tesseract-OCR\\\\tesseract.exe\"\n",
    "base_dir = os.path.abspath(os.path.dirname(\"__file__\"))\n",
    "\n",
    "# Different Proxys : =    192.41.71.221:3128   50.246.120.125:8080     \n",
    "PROXY = \"192.41.71.199:3128\"\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "appState = {\n",
    "    \"recentDestinations\": [\n",
    "        {\n",
    "            \"id\": \"Save as PDF\",\n",
    "            \"origin\": \"local\",\n",
    "            \"account\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    \"selectedDestinationId\": \"Save as PDF\",\n",
    "    \"version\": 2\n",
    "}\n",
    "profile = {'printing.print_preview_sticky_settings.appState': json.dumps(appState)}\n",
    "\n",
    "options.add_experimental_option('prefs',  {\n",
    "    \"download.default_directory\": base_dir,\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "    }\n",
    ")\n",
    "options.headless = False \n",
    "# options.add_argument('--proxy-server=%s' % PROXY)\n",
    "options.add_experimental_option('prefs', profile)\n",
    "options.add_argument('--kiosk-printing')\n",
    "path = \"C:\\\\Users\\\\Dishant\\\\Desktop\\\\chromedriver\\\\chromedriver\"\n",
    "global driver \n",
    "driver = webdriver.Chrome(executable_path = path , options = options)\n",
    "\n",
    "#Step1\n",
    "login() \n",
    "\n",
    "#Step2\n",
    "collect_data_from_search() \n",
    "time.sleep(10)\n",
    "\n",
    "#Step3\n",
    "global raw_data , case_date , case_type\n",
    "try:\n",
    "    raw_data = []\n",
    "    case_date = []\n",
    "    case_type = []\n",
    "    for i in range(len(data_row)):\n",
    "        driver.find_element_by_xpath(\"//*[@id='gridSearchResults']/tbody/tr[\"+str(i+1)+\"]/td[3]/a\").click()   \n",
    "        #Web page load completely\n",
    "        time.sleep(10)\n",
    "        data = driver.execute_script(\"return document.getElementsByClassName('pull-left')[0].innerText\")\n",
    "        time.sleep(3)\n",
    "        Case_Date = driver.execute_script(\"return document.getElementsByClassName('clerkfiledate')[1].innerText\")\n",
    "        time.sleep(3)\n",
    "        Case_Type = driver.execute_script(\"return document.getElementsByClassName('casetype')[1].innerText\")\n",
    "        time.sleep(3)\n",
    "        raw_data.append(data)\n",
    "        case_date.append(Case_Date)\n",
    "        case_type.append(Case_Type)\n",
    "        time.sleep(2)\n",
    "        #to go back to previous page\n",
    "        driver.execute_script(\"window.history.go(-1)\")\n",
    "        #to load the page with all classes\n",
    "        time.sleep(8)\n",
    "    print(len(raw_data) , len(case_date) , len(case_type))\n",
    "except Exception as e:\n",
    "    print(\"Error at Step 3 = \",e)\n",
    "\n",
    "#Cleanning Raw Data\n",
    "Clean_raw_data()\n",
    "\n",
    "#Cleanning case_date\n",
    "global new_case_date\n",
    "new_case_date = []\n",
    "for i in case_date:\n",
    "    demo = re.search(\"\\d{1,2}\\/\\d{1,2}\\/\\d{1,4}\",i).group()\n",
    "    mm = demo.split(\"/\")[0]\n",
    "    dd = demo.split(\"/\")[1]\n",
    "    yy = demo.split(\"/\")[2]\n",
    "    new_case_date.append(dd+\"/\"+mm+\"/\"+yy)\n",
    "print(len(new_case_date))\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "#Creting Dataset\n",
    "dataset = pd.DataFrame({'CaseId':case_id,'CaseDate':new_case_date,'CaseType':case_type,\n",
    "                        'BankName':bank_name,'DefendentName':def_name})\n",
    "#Creating a trial CSV\n",
    "dataset.to_csv('Martin_County_Try_Data.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def login():\n",
    "    \"\"\" LOGIN INTO THE WEBSITE WITH ALL DETAILS \"\"\"\n",
    "    try:\n",
    "        driver.get(\"https://court.martinclerk.com\")\n",
    "        #To load Website Completely\n",
    "        time.sleep(3)\n",
    "        #To enter the dates from and to\n",
    "        DateFrom = driver.find_element_by_id('openedFrom')\n",
    "        DateFrom.click()\n",
    "        DateFrom.send_keys(\"03162020\")\n",
    "\n",
    "        DateTo = driver.find_element_by_id('openedTo')\n",
    "        DateTo.click()\n",
    "        DateTo.send_keys(\"03222020\")\n",
    "\n",
    "        driver.find_element_by_xpath(\"//*[@id='courTypesButton']\").click()\n",
    "        driver.find_element_by_xpath(\"//*[@id='courTypesMultiSelect']/ul/li[1]/a/label/input\").click()\n",
    "        time.sleep(2) \n",
    "        driver.find_element_by_xpath(\"//*[@id='courTypesMultiSelect']/ul/li[3]/a/label/input\").click()\n",
    "\n",
    "        #To Get the next DropDownList\n",
    "        time.sleep(10)\n",
    "        driver.find_element_by_xpath(\"//*[@id='courTypesButton']\").click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        driver.find_element_by_xpath(\"//*[@id='caseTypesButton']\").click()\n",
    "        driver.find_element_by_xpath(\"//*[@id='caseTypesMultiSelect']/ul/li[1]/a/label/input\").click()\n",
    "        time.sleep(1)\n",
    "        #To select the Categories\n",
    "        for i in range(69,78):\n",
    "            driver.find_element_by_xpath(\"//*[@id='caseTypesMultiSelect']/ul/li[\"+str(i)+\"]/a/label/input\").click()\n",
    "\n",
    "        Captcha_Entry_Box = driver.find_element_by_xpath(\"//*[@id='mainTableContent']/tbody/tr/td/table/tbody/tr[2]/td[2]/div/div[3]/form/input[2]\")\n",
    "        Captcha_Entry_Box.click()\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        captcha_additon = int(input(\"Enter the additon check the headless browser :\\t\"))\n",
    "        Captcha_Entry_Box.send_keys(str(captcha_additon))\n",
    "\n",
    "        #To click on Search Button\n",
    "        Search_Button = driver.find_element_by_xpath(\"//*[@id='searchButton']\")\n",
    "        Search_Button.click()\n",
    "\n",
    "        #To Load all the classes on webpage\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        print(\"Error at Login() = \",e)\n",
    "        print(\"BURN THE CODE AGAIN....!!\")\n",
    "\n",
    "def collect_data_from_search():\n",
    "    \"\"\" COLLECT DATA FROM SEARCH RESULT TO USE FOR ITERATIONS \"\"\"\n",
    "    try:\n",
    "        table = driver.find_element_by_id(\"gridSearchResults\")\n",
    "        rows = table.find_elements_by_tag_name(\"tr\")\n",
    "\n",
    "        global data_row\n",
    "        data_row = []\n",
    "        for index in range(1,len(rows)):\n",
    "            data_row.append(rows[index].text)\n",
    "        print(\"Lenght of Data Collected From Rows = \",len(data_row))\n",
    "    except Exception as e:\n",
    "        print(\"Error at collect_data_from_search() = \",e)\n",
    "        \n",
    "\n",
    "def pdf_claim_value():\n",
    "    \"\"\" TO EXTRACT CLAIM VALUE \"\"\"\n",
    "    try:\n",
    "\n",
    "        #Using GLOB module\n",
    "        fn = glob.glob(\"C:\\\\Users\\\\Dishant\\\\Downloads\\\\*.pdf\")\n",
    "        FN = fn[0]\n",
    "        print(\"Filename = \",FN)\n",
    "        filename = FN\n",
    "        pages = convert_from_path(filename, 500)\n",
    "        print(\"Number of Pages found in PDF = \",len(pages))\n",
    "        matches = \"\"\n",
    "        for page in pages:\n",
    "            page.save('C:\\\\Users\\\\Dishant\\\\Downloads\\\\property_value.jpg', 'JPEG')\n",
    "            global property_value\n",
    "            property_value = str(((pytesseract.image_to_string(Image.open('C:\\\\Users\\\\Dishant\\\\Downloads\\\\property_value.jpg'))))).lower()\n",
    "            #print(property_value)\n",
    "            try:\n",
    "                matches = re.search(\"(\\\\d+\\\\W)*total estimated value of claim(\\\\W*\\\\d*)+\", property_value).group()\n",
    "                matches = matches.split(\"\\n\")\n",
    "                if \"total\" in matches[0]:\n",
    "                    matches = matches [0]\n",
    "                else:\n",
    "                    matches = matches[1]\n",
    "                #print(matches, \"1\")\n",
    "                claim_value = re.search(\"(\\\\d+\\\\W)+\", matches).group()\n",
    "                #print(claim_value)\n",
    "            except Exception as e:\n",
    "                claim_value=\"0\"\n",
    "                print(\"pdf_claim_value() error = \",e)\n",
    "        # Appending claim_value to CVL list \n",
    "        if(claim_value==\"0\"):\n",
    "            CVL.append(\"0\")\n",
    "            print(\"ClaimValue = \",claim_value)\n",
    "        else:\n",
    "            CVL.append(claim_value)\n",
    "            print(\"ClaimValue = \",claim_value)\n",
    "        #print(\"ITERATION  at pdf_claim_value() = \",CASE_NUM)\n",
    "        os.remove('C:\\\\Users\\\\Dishant\\\\Downloads\\\\property_value.jpg')\n",
    "        os.remove(filename)\n",
    "    except Exception as e:\n",
    "        print(\"Error at pdf_claim_value()\",e)\n",
    "        \n",
    "        \n",
    "\n",
    "\"\"\"FOR CLAIM VALUE CONVERTING TO INTEGER\"\"\"\n",
    "def claim_val_converter(string_value):\n",
    "    C_Value = \"\"\n",
    "    try:\n",
    "        for i in string_value.split(\".\")[0]: \n",
    "            if(i.isdigit()):\n",
    "\n",
    "                C_Value = C_Value + i\n",
    "        return int(C_Value)\n",
    "    except Exception as e:\n",
    "        C_Value = \"NA\"\n",
    "        return C_Value\n",
    "\n",
    "    \n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from pdf2image import convert_from_path\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from PIL import Image \n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "import pyautogui\n",
    "import requests\n",
    "import random\n",
    "import PyPDF2 \n",
    "import json\n",
    "import mouse\n",
    "import time \n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\\\Program Files (x86)\\\\Tesseract-OCR\\\\tesseract.exe\"\n",
    "base_dir = os.path.abspath(os.path.dirname(\"__file__\"))\n",
    "\n",
    "# Different Proxys : =    192.41.71.221:3128   50.246.120.125:8080     \n",
    "PROXY = \"192.41.71.199:3128\"\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "appState = {\n",
    "    \"recentDestinations\": [\n",
    "        {\n",
    "            \"id\": \"Save as PDF\",\n",
    "            \"origin\": \"local\",\n",
    "            \"account\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    \"selectedDestinationId\": \"Save as PDF\",\n",
    "    \"version\": 2\n",
    "}\n",
    "profile = {'printing.print_preview_sticky_settings.appState': json.dumps(appState)}\n",
    "\n",
    "options.add_experimental_option('prefs',  {\n",
    "    \"download.default_directory\": base_dir,\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "    }\n",
    ")\n",
    "options.headless = False \n",
    "# options.add_argument('--proxy-server=%s' % PROXY)\n",
    "options.add_experimental_option('prefs', profile)\n",
    "options.add_argument('--kiosk-printing')\n",
    "path = \"C:\\\\Users\\\\Dishant\\\\Desktop\\\\chromedriver\\\\chromedriver\"\n",
    "global driver \n",
    "driver = webdriver.Chrome(executable_path = path , options = options)\n",
    "\n",
    "#Step1\n",
    "login() \n",
    "\n",
    "#Step2\n",
    "collect_data_from_search() \n",
    "time.sleep(10)\n",
    "\n",
    "#Step3\n",
    "global CVL\n",
    "CVL = []\n",
    "try:\n",
    "    \n",
    "    for i in range(len(data_row)):\n",
    "        driver.find_element_by_xpath(\"//*[@id='gridSearchResults']/tbody/tr[\"+str(i+1)+\"]/td[3]/a\").click()   \n",
    "        #Web page load completely\n",
    "        time.sleep(10)\n",
    "        # PDF\n",
    "        # Searching For PDF\n",
    "        table = driver.find_element_by_id(\"gridDockets\")\n",
    "        rows = table.find_elements_by_tag_name(\"tr\")\n",
    "\n",
    "        # Gathering Data From GRID_DOCKETS {PDF search}\n",
    "        gridDockets = []\n",
    "        for i in range(1,len(rows)):\n",
    "            gridDockets.append(rows[i].text)\n",
    "        print(\"Lenght of Data Collected From Rows = \",len(gridDockets))\n",
    "        # Searching for PDF present or not\n",
    "        # imgCol\n",
    "        for caught,data in enumerate(gridDockets):\n",
    "            if ( \"VALUE OF REAL PROPERTY OR MORTGAGE FORECLOSURE CLAIM:\" in data ):\n",
    "                print(\"PDF Found index = \",caught)\n",
    "                break\n",
    "\n",
    "        # Buttoned image\n",
    "        x=0\n",
    "        for i in range(caught):\n",
    "            try:\n",
    "                re.search(\"\\d{1}\\s\\s\\s\\d{1,2}\\/\\d{1,2}\\/\\d{4}\",gridDockets[i]).group()\n",
    "                img = True\n",
    "                x+=1\n",
    "                #print(\"True\",x)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        #Clicking on the PDF imaged button\n",
    "        javaScript = 'document.getElementsByClassName(\"icon-fixed-width icon-large icon-file-alt\")['+str(x+1)+'].click();'\n",
    "        driver.execute_script(javaScript)\n",
    "\n",
    "        time.sleep(2)\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        time.sleep(2)\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        time.sleep(15)\n",
    "        pyautogui.moveTo(1319, 38)\n",
    "        time.sleep(5)\n",
    "        pyautogui.moveTo(1212, 176)\n",
    "        pyautogui.click()\n",
    "        time.sleep(1)\n",
    "        pyautogui.press('enter')\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        time.sleep(2)\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        # To get back to search results\n",
    "        driver.execute_script(\"window.history.go(-1)\")\n",
    "        #to load the page with all classes\n",
    "        time.sleep(8)\n",
    "        # Calling PDF_claim_value_function\n",
    "        pdf_claim_value()\n",
    "        time.sleep(3)\n",
    "        print(\"Iteration = \",i)\n",
    "except Exception as e:\n",
    "    print(\"Error at Step 3 = \",e)\n",
    "    CVL.append(\"NO PDF\")\n",
    "    print(\"Iteration = \",i)\n",
    "\n",
    "driver.quit()\n",
    "#Updating and Creating New Dataset\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"Martin_County_Try_Data.csv\")\n",
    "del dataset[\"Unnamed: 0\"]\n",
    "dataset[\"RealValueString\"] = CVL\n",
    "#RealValue\n",
    "global RV\n",
    "RV=[]\n",
    "for i in CVL:\n",
    "    RV.append(claim_val_converter(i))\n",
    "dataset[\"RealValue\"] = RV\n",
    "os.remove('Martin_County_Try_Data.csv')\n",
    "dataset.to_csv('Martin_County_Try_Data.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def login():\n",
    "    \"\"\" LOGIN INTO THE WEBSITE WITH ALL DETAILS \"\"\"\n",
    "    try:\n",
    "        driver.get(\"https://court.martinclerk.com\")\n",
    "        #To load Website Completely\n",
    "        time.sleep(3)\n",
    "        #To enter the dates from and to\n",
    "        DateFrom = driver.find_element_by_id('openedFrom')\n",
    "        DateFrom.click()\n",
    "        DateFrom.send_keys(\"03162020\")\n",
    "\n",
    "        DateTo = driver.find_element_by_id('openedTo')\n",
    "        DateTo.click()\n",
    "        DateTo.send_keys(\"03222020\")\n",
    "\n",
    "        driver.find_element_by_xpath(\"//*[@id='courTypesButton']\").click()\n",
    "        driver.find_element_by_xpath(\"//*[@id='courTypesMultiSelect']/ul/li[1]/a/label/input\").click()\n",
    "        time.sleep(2) \n",
    "        driver.find_element_by_xpath(\"//*[@id='courTypesMultiSelect']/ul/li[3]/a/label/input\").click()\n",
    "\n",
    "        #To Get the next DropDownList\n",
    "        time.sleep(10)\n",
    "        driver.find_element_by_xpath(\"//*[@id='courTypesButton']\").click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        driver.find_element_by_xpath(\"//*[@id='caseTypesButton']\").click()\n",
    "        driver.find_element_by_xpath(\"//*[@id='caseTypesMultiSelect']/ul/li[1]/a/label/input\").click()\n",
    "        time.sleep(1)\n",
    "        #To select the Categories\n",
    "        for i in range(69,78):\n",
    "            driver.find_element_by_xpath(\"//*[@id='caseTypesMultiSelect']/ul/li[\"+str(i)+\"]/a/label/input\").click()\n",
    "\n",
    "        Captcha_Entry_Box = driver.find_element_by_xpath(\"//*[@id='mainTableContent']/tbody/tr/td/table/tbody/tr[2]/td[2]/div/div[3]/form/input[2]\")\n",
    "        Captcha_Entry_Box.click()\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        captcha_additon = int(input(\"Enter the additon check the headless browser :\\t\"))\n",
    "        Captcha_Entry_Box.send_keys(str(captcha_additon))\n",
    "\n",
    "        #To click on Search Button\n",
    "        Search_Button = driver.find_element_by_xpath(\"//*[@id='searchButton']\")\n",
    "        Search_Button.click()\n",
    "\n",
    "        #To Load all the classes on webpage\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        print(\"Error at Login() = \",e)\n",
    "        print(\"BURN THE CODE AGAIN....!!\")\n",
    "\n",
    "def collect_data_from_search():\n",
    "    \"\"\" COLLECT DATA FROM SEARCH RESULT TO USE FOR ITERATIONS \"\"\"\n",
    "    try:\n",
    "        table = driver.find_element_by_id(\"gridSearchResults\")\n",
    "        rows = table.find_elements_by_tag_name(\"tr\")\n",
    "\n",
    "        global data_row\n",
    "        data_row = []\n",
    "        for index in range(1,len(rows)):\n",
    "            data_row.append(rows[index].text)\n",
    "        print(\"Lenght of Data Collected From Rows = \",len(data_row))\n",
    "    except Exception as e:\n",
    "        print(\"Error at collect_data_from_search() = \",e)\n",
    "\n",
    "\n",
    "\"\"\"FOR CLAIM VALUE CONVERTING TO INTEGER\"\"\"\n",
    "def claim_val_converter(string_value):\n",
    "    C_Value = \"\"\n",
    "    try:\n",
    "        for i in string_value.split(\".\")[0]: \n",
    "            if(i.isdigit()):\n",
    "\n",
    "                C_Value = C_Value + i\n",
    "        return int(C_Value)\n",
    "    except Exception as e:\n",
    "        C_Value = \"NA\"\n",
    "        return C_Value\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"CODE FOR PDF address extraction\"\"\"\n",
    "def PDFadd():\n",
    "    try: \n",
    "        global add\n",
    "        #Using GLOB module\n",
    "        fn = glob.glob(\"C:\\\\Users\\\\Dishant\\\\Downloads\\\\*.pdf\")\n",
    "        FN = fn[0]\n",
    "        print(\"Filename = \",FN)\n",
    "        filename = FN       \n",
    "        pages = convert_from_path(filename, 500)\n",
    "        print(\"\\nNumber of Pages found in PDF = \",len(pages))\n",
    "        trials=0\n",
    "        start = \"\"\n",
    "        end = \"\"\n",
    "        add=\"\"\n",
    "        found = False\n",
    "        for page in pages:\n",
    "            add=\"\"\n",
    "            page.save('C:\\\\Users\\\\Dishant\\\\Downloads\\\\add_img.jpg', 'JPEG')\n",
    "            address_txt = str(((pytesseract.image_to_string(Image.open('C:\\\\Users\\\\Dishant\\\\Downloads\\\\add_img.jpg'))))).lower()\n",
    "            try:\n",
    "                add=\"\"\n",
    "                # If UNKNOWN is there \n",
    "                start = re.search(r'unknown',address_txt).span()\n",
    "                end = address_txt[start[1]:]\n",
    "                Unkonwn_Found = True\n",
    "                if(Unkonwn_Found):\n",
    "                    try:\n",
    "                        add=\"\"\n",
    "                        print(\"UNKNOWN FOUND\")\n",
    "                        ############ First Attempt\n",
    "                        try:\n",
    "                            add = re.search(\"\\d{2,5}\\s[\\b\\w,@\\s]*\\sfl\\s\\d{4,5}\\-\\d{3,5}\", end).group()\n",
    "                            print(add)\n",
    "                            print(\"Address_Found in Attempt = 1\")\n",
    "                            print(\"1\")\n",
    "                            comp_address.append(add)\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            add = re.search(r'\\d{2,5}\\s[\\b\\w,@\\s]*\\sfl\\s\\d{4,5}', end).group()\n",
    "                            print(add)\n",
    "                            print(\"Address_Found in (Exception)Attempt = 1\")\n",
    "                            print(\"2\")\n",
    "                            comp_address.append(add)\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        add=\"\"\n",
    "                        ############ Second Attempt\n",
    "                        try:\n",
    "                            add = re.search(\"\\d{2,5}\\s[\\b\\w,@\\s]*\\sflorida\\s\\d{4,5}\\-\\d{3,5}\", end).group()\n",
    "                            print(add)\n",
    "                            print(\"Address_Found in Attempt = 2\")\n",
    "                            print(\"3\")\n",
    "                            comp_address.append(add)\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            add = re.search(\"\\d{2,5}\\s[\\b\\w,@\\s]*\\sflorida\\s\\d{4,5}\", end).group()\n",
    "                            print(add)\n",
    "                            print(\"Address_Found in (Exception)Attempt = 2\")\n",
    "                            print(\"4\")\n",
    "                            comp_address.append(add)\n",
    "                            break\n",
    "\n",
    "            except Exception as e:\n",
    "                add=\"\"\n",
    "                try:\n",
    "                    add=\"\"\n",
    "                    print(\"UNKNOWN NOT FOUND\")\n",
    "                    ############ First Attempt\n",
    "                    try:\n",
    "                        add = re.search(\"\\d{2,5}\\s[\\b\\w,@\\s]*\\sfl\\s\\d{4,5}\\-\\d{3,5}\", address_txt).group()\n",
    "                        print(add)\n",
    "                        print(\"Address_Found in Attempt = 1\")\n",
    "                        print(\"5\")\n",
    "                        comp_address.append(add)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        add = re.search(r'\\d{2,5}\\s[\\b\\w,@\\s]*\\sfl\\s\\d{4,5}', address_txt).group()\n",
    "                        print(add)\n",
    "                        print(\"Address_Found in (Exception)Attempt = 1\")\n",
    "                        print(\"6\")\n",
    "                        comp_address.append(add)\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    add=\"\"\n",
    "                    ############ Second Attempt\n",
    "                    try:\n",
    "                        add = re.search(\"\\d{2,5}\\s[\\b\\w,@\\s]*\\sflorida\\s\\d{4,5}\\-\\d{3,5}\", address_txt).group()\n",
    "                        print(add)\n",
    "                        print(\"Address_Found in Attempt = 2\")\n",
    "                        print(\"7\")\n",
    "                        comp_address.append(add)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        add = re.search(\"\\d{2,5}\\s[\\b\\w,@\\s]*\\sflorida\\s\\d{4,5}\", address_txt).group()\n",
    "                        print(add)\n",
    "                        print(\"Address_Found in (Exception)Attempt = 2\")\n",
    "                        print(\"8\")\n",
    "                        comp_address.append(add)\n",
    "                        break\n",
    "    except Exception as e:\n",
    "        print(\"ERROR\")\n",
    "        # If add is empty .....  \n",
    "    if(add==\"\"):\n",
    "        comp_address.append(\"NAF\")\n",
    "    os.remove('C:\\\\Users\\\\Dishant\\\\Downloads\\\\add_img.jpg')\n",
    "    os.remove(filename)\n",
    "    #print(\"ITERATION at PDFadd() exception = \",CASE_NUM)\n",
    "    #os.close(filename)\n",
    "\n",
    "    \n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from pdf2image import convert_from_path\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from PIL import Image \n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "import pyautogui\n",
    "import requests\n",
    "import random\n",
    "import PyPDF2 \n",
    "import json\n",
    "import mouse\n",
    "import time \n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\\\Program Files (x86)\\\\Tesseract-OCR\\\\tesseract.exe\"\n",
    "base_dir = os.path.abspath(os.path.dirname(\"__file__\"))\n",
    "\n",
    "# Different Proxys : =    192.41.71.221:3128   50.246.120.125:8080     \n",
    "PROXY = \"192.41.71.199:3128\"\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "appState = {\n",
    "    \"recentDestinations\": [\n",
    "        {\n",
    "            \"id\": \"Save as PDF\",\n",
    "            \"origin\": \"local\",\n",
    "            \"account\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    \"selectedDestinationId\": \"Save as PDF\",\n",
    "    \"version\": 2\n",
    "}\n",
    "profile = {'printing.print_preview_sticky_settings.appState': json.dumps(appState)}\n",
    "\n",
    "options.add_experimental_option('prefs',  {\n",
    "    \"download.default_directory\": base_dir,\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "    }\n",
    ")\n",
    "options.headless = False \n",
    "# options.add_argument('--proxy-server=%s' % PROXY)\n",
    "options.add_experimental_option('prefs', profile)\n",
    "options.add_argument('--kiosk-printing')\n",
    "path = \"C:\\\\Users\\\\Dishant\\\\Desktop\\\\chromedriver\\\\chromedriver\"\n",
    "global driver \n",
    "driver = webdriver.Chrome(executable_path = path , options = options)\n",
    "\n",
    "#Step1\n",
    "login() \n",
    "\n",
    "#Step2\n",
    "collect_data_from_search() \n",
    "time.sleep(10)\n",
    "\n",
    "\n",
    "# taking values from dataset\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"Martin_County_Try_Data.csv\")\n",
    "\n",
    "\n",
    "#Step3\n",
    "global comp_address\n",
    "comp_address = []\n",
    "try:\n",
    "    \n",
    "    for xyz in range(len(data_row)):\n",
    "        driver.find_element_by_xpath(\"//*[@id='gridSearchResults']/tbody/tr[\"+str(xyz+1)+\"]/td[3]/a\").click()   \n",
    "        #Web page load completely\n",
    "        time.sleep(10)\n",
    "        # PDF\n",
    "        # Searching For PDF\n",
    "        table = driver.find_element_by_id(\"gridDockets\")\n",
    "        rows = table.find_elements_by_tag_name(\"tr\")\n",
    "\n",
    "        # Gathering Data From GRID_DOCKETS {PDF search}\n",
    "        gridDockets = []\n",
    "        for i in range(1,len(rows)):\n",
    "            gridDockets.append(rows[i].text)\n",
    "        print(\"Lenght of Data Collected From Rows = \",len(gridDockets))\n",
    "\n",
    "        # Searching for PDF present or not\n",
    "        # imgCol\n",
    "        for caught,data in enumerate(gridDockets):\n",
    "            if ( \"SUMMONS ISSUED TO\" in data ):\n",
    "                print(\"PDF Found index = \",caught)\n",
    "                break\n",
    "\n",
    "        # Buttoned image\n",
    "        x=0\n",
    "        for i in range(caught):\n",
    "            try:\n",
    "                re.search(\"\\d{1}\\s\\s\\s\\d{1,2}\\/\\d{1,2}\\/\\d{4}\",gridDockets[i]).group()\n",
    "                img = True\n",
    "                x+=1\n",
    "                #print(\"True\",x)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        #Clicking on the PDF imaged button\n",
    "        javaScript = 'document.getElementsByClassName(\"icon-fixed-width icon-large icon-file-alt\")['+str(x+1)+'].click();'\n",
    "        driver.execute_script(javaScript)\n",
    "\n",
    "        time.sleep(2)\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        time.sleep(2)\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        time.sleep(60)\n",
    "        pyautogui.moveTo(1319, 38)\n",
    "        time.sleep(5)\n",
    "        pyautogui.moveTo(1212, 176)\n",
    "        pyautogui.click()\n",
    "        time.sleep(1)\n",
    "        pyautogui.press('enter')\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        time.sleep(2)\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        # To get back to search results\n",
    "        driver.execute_script(\"window.history.go(-1)\")\n",
    "        #to load the page with all classes\n",
    "        time.sleep(8)\n",
    "\n",
    "        # Collecting Address\n",
    "        PDFadd()\n",
    "        \n",
    "        time.sleep(3)\n",
    "        print(\"Iteration = \",xyz)\n",
    "except Exception as e:\n",
    "    print(\"Error at Step 3 = \",e)\n",
    "    comp_address.append(\"NAF\")\n",
    "    print(\"Iteration = \",xyz)\n",
    "\n",
    "#Creating new DataSet\n",
    "\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"Martin_County_Try_Data.csv\")\n",
    "del dataset[\"Unnamed: 0\"]\n",
    "dataset[\"Comp_Address\"] = comp_address\n",
    "os.remove('Martin_County_Try_Data.csv')\n",
    "dataset.to_csv('Martin_County_Try_Data.csv')\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"FOR GOING TO WESITE FOR PROPERTY VALUE\"\"\"\n",
    "def prop_val(x,y,z):\n",
    "    try:\n",
    "        driver.get(\"http://www.totalviewrealestate.com/\")  \n",
    "        time.sleep(10)   \n",
    "        driver.find_element_by_xpath('//*[@id=\"header\"]/form/p/input[1]').send_keys(x) \n",
    "        time.sleep(2)\n",
    "        driver.find_element_by_xpath('//*[@id=\"header\"]/form/p/input[2]').send_keys(y) \n",
    "        time.sleep(2)\n",
    "        driver.find_element_by_xpath('//*[@id=\"header\"]/form/p/input[3]').send_keys(z) \n",
    "        time.sleep(4)\n",
    "        driver.find_element_by_xpath('//*[@id=\"header\"]/form/input').click() #SUBMIT\n",
    "        time.sleep(17)\n",
    "        data = driver.find_element_by_xpath('//*[@id=\"leftpi\"]/p[2]').text\n",
    "        print(data)\n",
    "        lis.append(data)\n",
    "        time.sleep(5)\n",
    "    except Exception as e:\n",
    "        print(\"Error at pop_val() = \",e)\n",
    "        lis.append(\"NAF\")\n",
    "\n",
    "\n",
    "\"\"\"FOR CLAIM VALUE CONVERTING TO INTEGER\"\"\"\n",
    "def claim_val_converter(string_value):\n",
    "    C_Value = \"\"\n",
    "    try:\n",
    "        for i in string_value.split(\".\")[0]: \n",
    "            if(i.isdigit()):\n",
    "\n",
    "                C_Value = C_Value + i\n",
    "        return int(C_Value)\n",
    "    except Exception as e:\n",
    "        C_Value = \"NA\"\n",
    "        return C_Value\n",
    "\n",
    "\n",
    "\"\"\"FOR HIGH VALUATION VALUE\"\"\"\n",
    "def HVVF():\n",
    "    high_value = \"\"\n",
    "    for i in HVS: \n",
    "        if(i.isdigit()):\n",
    "            high_value = high_value + i\n",
    "    return high_value\n",
    "\n",
    "\"\"\"FOR LOW VALUATION VALUE\"\"\"\n",
    "def LVVF():\n",
    "    low_value = \"\"\n",
    "    for i in LVS: \n",
    "        if(i.isdigit()):\n",
    "            low_value = low_value + i\n",
    "    return low_value\n",
    "         \n",
    "    \n",
    "#######################################################\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from pdf2image import convert_from_path\n",
    "# import win32com.client as wincl       # Speech Module\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from PIL import Image \n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "import pyautogui\n",
    "import requests\n",
    "import random\n",
    "import PyPDF2 \n",
    "import json\n",
    "import mouse\n",
    "import time \n",
    "import os\n",
    "import re\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\\\Program Files (x86)\\\\Tesseract-OCR\\\\tesseract.exe\"\n",
    "base_dir = os.path.abspath(os.path.dirname(\"__file__\"))\n",
    "\n",
    "# Different Proxys : =    192.41.71.199:3128   50.246.120.125:8080      \n",
    "PROXY =   \"138.197.32.120:3128\"  #\"157.230.44.213:8080\"  #\"138.197.32.120:3128\"  #\"74.121.98.90:8080\"  \n",
    "            #\"192.144.215.244:3128\"  #\"157.230.44.213:8080\"   #\"74.121.98.90:8080\"    #\"192.41.71.221:3128\"\n",
    "options = webdriver.ChromeOptions()\n",
    "appState = {\n",
    "    \"recentDestinations\": [\n",
    "        {\n",
    "            \"id\": \"Save as PDF\",\n",
    "            \"origin\": \"local\",\n",
    "            \"account\": \"\"\n",
    "        }\n",
    "    ],\n",
    "    \"selectedDestinationId\": \"Save as PDF\",\n",
    "    \"version\": 2\n",
    "}\n",
    "profile = {'printing.print_preview_sticky_settings.appState': json.dumps(appState)}\n",
    "options.add_experimental_option('prefs',  {\n",
    "    \"download.default_directory\": base_dir,\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "    }\n",
    ")\n",
    "options.headless = False #True  \n",
    "options.add_argument('--proxy-server=%s' % PROXY)\n",
    "options.add_experimental_option('prefs', profile)\n",
    "options.add_argument('--kiosk-printing')\n",
    "path = \"C:\\\\Users\\\\Dishant\\\\Desktop\\\\chromedriver\\\\chromedriver\"\n",
    "\n",
    "global driver , lis\n",
    "lis = []\n",
    "\n",
    "driver = webdriver.Chrome(executable_path = path , options = options)\n",
    "\n",
    "\"\"\"MAIN PROGRAM\"\"\"\n",
    "#Reading Address from Dataset\n",
    "dataset = pd.read_csv(\"Martin_County_Try_Data.csv\")\n",
    "dataset.head(6)\n",
    "\n",
    "new = []\n",
    "for i in dataset[\"Comp_Address\"]: new.append(i)\n",
    "print(len(new))\n",
    "########### Sending data to get ratios and values for calculation\n",
    "for i in range(len(new)):\n",
    "#     print(new[i])\n",
    "    if(new[i]==\"NAF\"):\n",
    "        lis.append(\"NAF\")\n",
    "        print(\"NAF\",\"iteration  = \",i)\n",
    "        continue\n",
    "    try:\n",
    "        try:\n",
    "            \n",
    "            demo = new[i].split(\"\\n\")\n",
    "            Address = demo[0]\n",
    "            City = demo[3].split(\",\")[0]\n",
    "            pin =  demo[3].split(\",\")[1].split(\" \")[2]\n",
    "            prop_val(Address,City,pin)\n",
    "            print(Address,\"|\",City,\"|\",pin,\"iteration  = \",i)\n",
    "            \n",
    "        except Exception as e:\n",
    "            \n",
    "            demo = new[i].split(\"\\n\")\n",
    "            Address = demo[0]\n",
    "            City = demo[1].split(\",\")[0]\n",
    "            pin =  demo[1].split(\",\")[1].split(\" \")[2]\n",
    "            prop_val(Address,City,pin)\n",
    "            print(Address,\"|\",City,\"|\",pin,\"iteration  = \",i)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Error = \",e,\"  iteration  = \",i)\n",
    "        lis.append(\"NAF\")\n",
    "        continue            \n",
    "driver.quit()\n",
    "\n",
    "\n",
    "#Cleaning the gathered data into four lists for Calculations      \n",
    "list_HVS = []\n",
    "list_HVV = []\n",
    "list_LVS = []\n",
    "list_LVV = []\n",
    "\n",
    "\"\"\" FOR EXTRACTING HIGH LOW VALUE OF PROPRERTY \"\"\"\n",
    "for i in range(len(lis)):\n",
    "    demo = lis[i]\n",
    "    try:\n",
    "        #print(demo)\n",
    "        start = re.search(\"Value\\sRange\",demo).span()\n",
    "        #print(start)\n",
    "        end = re.search(\"Change\\:\\s\",demo).span()\n",
    "        #print(end)\n",
    "        got = demo[start[1]:end[0]]\n",
    "        got.split(\"to\")\n",
    "        HVS = got.split(\"to\")[1].split(\"\\n\")[0]\n",
    "        HVV = HVVF()\n",
    "        LVS = got.split(\"to\")[0].split(\" \")[1]\n",
    "        LVV = LVVF()\n",
    "        print(\"\\n\")\n",
    "        print(\"Low Value String = \",LVS)\n",
    "        print(\"Low Value = \",LVV)\n",
    "        print(\"High Value String = \",HVS)\n",
    "        print(\"High Value = \",HVV)\n",
    "        list_HVS.append(HVS)\n",
    "        list_HVV.append(int(HVV))\n",
    "        list_LVS.append(LVS)\n",
    "        list_LVV.append(int(LVV))\n",
    "    except Exception as e:\n",
    "        HVS = \"NA\"\n",
    "        HVV = \"NA\"\n",
    "        LVS = \"NA\"\n",
    "        LVV = \"NA\"\n",
    "        print(\"\\n\")\n",
    "        print(\"Low Value String = \",LVS)\n",
    "        print(\"Low Value = \",LVV)\n",
    "        print(\"High Value String = \",HVS)\n",
    "        print(\"High Value = \",HVV)\n",
    "        list_HVS.append(HVS)\n",
    "        list_HVV.append(HVV)\n",
    "        list_LVS.append(LVS)\n",
    "        list_LVV.append(LVV)\n",
    "        \n",
    "# print(len(list_HVS))\n",
    "# print(len(list_LVS))\n",
    "# print(len(list_HVV))\n",
    "# print(len(list_LVV))\n",
    "\n",
    "# \"\"\"FOR CALCULATIONS\"\"\"\n",
    "RV = []\n",
    "for i in dataset[\"RealValue\"]: RV.append(i)\n",
    "print(len(RV))\n",
    "\n",
    "Avg_App_V_L=[]\n",
    "Avg_Est_V_L = []\n",
    "Acq_Ratio_L = []\n",
    "\n",
    "\n",
    "for i in range(len(RV)):\n",
    "    try:\n",
    "        real_value = RV[i]\n",
    "        high = list_HVV[i]\n",
    "        #print(high)\n",
    "        low = list_LVV[i]\n",
    "        #print(low)\n",
    "\n",
    "        if(high==\"NA\" or low == \"NA\"):\n",
    "            Avg_App_V = \"NA\"   \n",
    "            Avg_App_V_L.append(Avg_App_V) #Average Approximate Value\n",
    "            Avg_Est_V = \"NA\" \n",
    "            Avg_Est_V_L.append(Avg_Est_V) #Average Estimated Value\n",
    "            Acq_Ratio = \"NA\"\n",
    "            Acq_Ratio_L.append(Acq_Ratio) #Acquisation Ratio\n",
    "        else:\n",
    "            Avg_App_V = (high+low)/2   \n",
    "            Avg_App_V_L.append(Avg_App_V) #Average Approximate Value\n",
    "            if(real_value == \"NA\"):\n",
    "                Avg_Est_V = \"NA\"\n",
    "                Avg_Est_V_L.append(Avg_Est_V) #Average Estimated Value\n",
    "                Acq_Ratio = \"NA\"\n",
    "                Acq_Ratio_L.append(Acq_Ratio) #Acquisation Ratio\n",
    "\n",
    "            else:\n",
    "                Avg_Est_V = Avg_App_V - real_value \n",
    "                Avg_Est_V_L.append(Avg_Est_V) #Average Estimated Value\n",
    "                Acq_Ratio = (real_value / Avg_App_V) * 100\n",
    "                Acq_Ratio_L.append(Acq_Ratio) #Acquisation Ratio\n",
    "    except Exception as e:\n",
    "        Avg_App_V = \"NA\"   \n",
    "        Avg_App_V_L.append(Avg_App_V) #Average Approximate Value\n",
    "        Avg_Est_V = \"NA\" \n",
    "        Avg_Est_V_L.append(Avg_Est_V) #Average Estimated Value\n",
    "        Acq_Ratio = \"NA\"\n",
    "        Acq_Ratio_L.append(Acq_Ratio) #Acquisation Ratio\n",
    "\n",
    "        \n",
    "\"\"\" Getting Address , City , Pincode \"\"\"\n",
    "new = []\n",
    "for i in dataset[\"Comp_Address\"]:\n",
    "    new.append(i)\n",
    "print(len(new))\n",
    "# new \n",
    "\n",
    "\"\"\"ADDING AQUIRED DATA TO DATAFRAME\"\"\"\n",
    "address = []\n",
    "city = []\n",
    "zipcode  = []\n",
    "\n",
    "for i in range(len(new)):\n",
    "    if(new[i]==\"0\"):\n",
    "        address.append(\"0\")\n",
    "        city.append(\"0\")\n",
    "        zipcode.append(\"0\")\n",
    "        continue\n",
    "    try:\n",
    "        try:\n",
    "\n",
    "            demo = new[i].split(\"\\n\")\n",
    "            Address = demo[0]\n",
    "            City = demo[3].split(\",\")[0]\n",
    "            pin =  demo[3].split(\",\")[1].split(\" \")[2]\n",
    "            address.append(Address)\n",
    "            city.append(City)\n",
    "            zipcode.append(\"FL \" + pin)\n",
    "            print(Address,\"|\",City,\"|\",pin,\"iteration  = \",i)            \n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            demo = new[i].split(\"\\n\")\n",
    "            Address = demo[0]\n",
    "            City = demo[1].split(\",\")[0]\n",
    "            pin =  demo[1].split(\",\")[1].split(\" \")[2]\n",
    "            address.append(Address)\n",
    "            city.append(City)\n",
    "            zipcode.append(\"FL \" + pin)\n",
    "            print(Address,\"|\",City,\"|\",pin,\"iteration  = \",i)   \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Error = \",e,\"  iteration  = \",i)\n",
    "        address.append(\"0\")\n",
    "        city.append(\"0\")\n",
    "        zipcode.append(\"0\")\n",
    "        continue\n",
    "\n",
    "# print(len(Avg_App_V_L),len(Avg_Est_V_L),len(Acq_Ratio_L))\n",
    "# print(len(address),len(city),len(zipcode))\n",
    "\n",
    "#Making new dataset\n",
    "del dataset[\"Unnamed: 0\"]\n",
    "dataset['Address'] = address\n",
    "dataset['City'] = city\n",
    "dataset['Zipcode'] = zipcode\n",
    "dataset['HighValuationString'] = list_HVS\n",
    "dataset['HighValuationValue'] = list_HVV\n",
    "dataset['LowValuationString'] = list_LVS\n",
    "dataset['LowValuationValue'] = list_LVV\n",
    "dataset['AverageApproximateValue'] = Avg_App_V_L\n",
    "dataset['AverageEstimateValue'] = Avg_Est_V_L\n",
    "dataset['AcquisitionRatio'] = Acq_Ratio_L\n",
    "del dataset[\"Comp_Address\"]\n",
    "county_name = []\n",
    "for i in range(len(lis)): county_name.append(\"Martin County\")\n",
    "dataset['CountyName'] = county_name\n",
    "\n",
    "os.remove(\"Martin_County_Try_Data.csv\")\n",
    "dataset.to_csv('Martin_County_Try_Data.csv')   # Final CSV File...\n",
    "dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point(x=1212, y=176) #download\n",
    "# Point(x=1319, y=38) #close\n",
    "# time.sleep(10)\n",
    "# pyautogui.hotkey(\"ctrlleft\", \"p\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
